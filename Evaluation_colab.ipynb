{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Evaluation_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "suz1J69hzolz",
        "jxiYT8zvlV0J",
        "eN2Onvarn_3H"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED6QCEVU6INK",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awtOsBP3WTL7",
        "colab_type": "code",
        "outputId": "1381400b-7215-4874-b896-169484d702a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--WEHKH0n6kl",
        "colab_type": "code",
        "outputId": "416dbee9-7f60-4c8a-c6ad-0e68063640f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import nltk\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import itertools,time\n",
        "import sys, os\n",
        "from collections import OrderedDict,Counter\n",
        "from copy import deepcopy\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "import sys, getopt\n",
        "import re\n",
        "import gensim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suz1J69hzolz",
        "colab_type": "text"
      },
      "source": [
        "##### Load 20 News Groups Dataset(Saved)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsNzWiloumtB",
        "colab_type": "code",
        "outputId": "6f9547f7-5958-4ab9-cdc6-cae856931ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnbwxTkxzOvv",
        "colab_type": "code",
        "outputId": "c4a1c494-bd3e-4c68-9cac-15505a124b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "newsgroups_train.data[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpNNUxqM02jT",
        "colab_type": "text"
      },
      "source": [
        "remove headers footers quotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hlWqtemziup",
        "colab_type": "code",
        "outputId": "0f652411-089f-4676-c93a-70a892112cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train_maintext = fetch_20newsgroups(subset='train',\n",
        "                                      remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test_maintext = fetch_20newsgroups(subset='test',\n",
        "                                      remove=('headers', 'footers', 'quotes'))\n",
        "print(newsgroups_train_maintext.data[0])\n",
        "print(newsgroups_test_maintext.data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "I am a little confused on all of the models of the 88-89 bonnevilles.\n",
            "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
            "differences are far as features or performance. I am also curious to\n",
            "know what the book value is for prefereably the 89 model. And how much\n",
            "less than book value can you usually get them for. In other words how\n",
            "much are they in demand this time of year. I have heard that the mid-spring\n",
            "early summer is the best time to buy.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUrbtgXPwAA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\n'.join(newsgroups_train.target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxiYT8zvlV0J",
        "colab_type": "text"
      },
      "source": [
        "#### Tokenize Dataset (Saved)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1c5dpe8hG4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import string\n",
        "\n",
        "tokenizer = spacy.load('en_core_web_sm')\n",
        "punctuations = string.punctuation\n",
        "# lowercase and remove punctuation\n",
        "def tokenize(sent):\n",
        "  tokens = tokenizer(sent)\n",
        "  return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
        "\n",
        "def tokenize_dataset(dataset):\n",
        "    token_dataset = []\n",
        "    # we are keeping track of all tokens in dataset \n",
        "    # in order to create vocabulary later\n",
        "    all_tokens = []\n",
        "    \n",
        "    for sample in dataset:\n",
        "        tokens = tokenize(sample)\n",
        "        token_dataset.append(tokens)\n",
        "        all_tokens += tokens\n",
        "\n",
        "    return token_dataset, all_tokens\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1Zh9euEmSl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_dataset_train, all_tokens_train = tokenize_dataset(newsgroups_train.data)\n",
        "pkl.dump(token_dataset_train, open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_train.p\", \"wb\"))\n",
        "pkl.dump(all_tokens_train, open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/all_tokens_20newsgroups_train.p\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL00Re56jGXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_dataset_test, all_tokens_test = tokenize_dataset(newsgroups_test.data)\n",
        "pkl.dump(token_dataset_test, open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_test.p\", \"wb\"))\n",
        "pkl.dump(all_tokens_test, open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/all_tokens_20newsgroups_test.p\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdCJoX1-0OS_",
        "colab_type": "text"
      },
      "source": [
        "=========================================================  \n",
        "remove header footers and quotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO9WgWjx0NEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_dataset_train, all_tokens_train = tokenize_dataset(newsgroups_train_maintext.data)\n",
        "pkl.dump(token_dataset_train, open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_maintext_train.p\", \"wb\"))\n",
        "pkl.dump(all_tokens_train, open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/all_tokens_20newsgroups_maintext_train.p\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cufNly1n0bzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_dataset_test, all_tokens_test = tokenize_dataset(newsgroups_test_maintext.data)\n",
        "pkl.dump(token_dataset_test, open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_maintext_test.p\", \"wb\"))\n",
        "pkl.dump(all_tokens_test, open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/all_tokens_20newsgroups_maintext_test.p\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN2Onvarn_3H",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocess and Clean Dataset (Saved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5BTQ87G4xzH",
        "colab_type": "text"
      },
      "source": [
        "###### load tokenized dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmTtenErmzdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with headers footers\n",
        "token_dataset_train = pkl.load(open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_train.p\", \"rb\"))\n",
        "all_tokens_train = pkl.load(open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/all_tokens_20newsgroups_train.p\", \"rb\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJCCoAjf4aS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#without headers footers\n",
        "token_dataset_train = pkl.load(open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_maintext_train.p\", \"rb\"))\n",
        "all_tokens_train = pkl.load(open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/all_tokens_20newsgroups_maintext_train.p\", \"rb\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwNCEsLi42lv",
        "colab_type": "text"
      },
      "source": [
        "###### preprocess funcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KcHB8a7l8sq",
        "colab_type": "code",
        "outputId": "c4801d6a-cfe0-4e3c-dad6-db49415b88ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufPvoxPml2MD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import itertools,time\n",
        "import sys, os\n",
        "from collections import OrderedDict,Counter\n",
        "from copy import deepcopy\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "import sys, getopt\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "wnl = WordNetLemmatizer() \n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "words = set(nltk.corpus.words.words())\n",
        "stops = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CENN5uVDGY3Z",
        "colab": {}
      },
      "source": [
        "\n",
        "def process_text(texts, lemmatization = None, with_bigram=False, min_length = None):\n",
        "    \"\"\"\n",
        "    Function to process texts. Following are the steps we take:\n",
        "    \n",
        "    1. Stopword Removal(nltk)\n",
        "    2. Lemmatization (not stem since stemming can reduce the interpretability).\n",
        "    3. Collocation detection. \n",
        "    \n",
        "    Parameters:\n",
        "    ----------\n",
        "    texts: Tokenized texts.\n",
        "    \n",
        "    Returns:\n",
        "    -------\n",
        "    texts: Pre-processed tokenized texts.\n",
        "    \"\"\"\n",
        "    # stopword removal using NLTK's english stopwords dataset.\n",
        "    texts = [[word for word in line if word not in stops and word.isalpha() ] for line in texts] \n",
        "    \n",
        "\n",
        "    # Lemmatization is generally better than stemming in the case of topic modeling \n",
        "    if lemmatization:\n",
        "        # texts = [[lemmatizer.lemmatize(word) for word in line] for line in texts] \n",
        "\n",
        "        texts = [[wnl.lemmatize(i,j[0].lower()) if j[0].lower() in lemmatization else wnl.lemmatize(i) for i,j in pos_tag(line)] for line in texts]\n",
        "\n",
        "   \n",
        "\n",
        "    # Bigram collocation detection (frequently co-occuring tokens) using gensim's Phrases. can even try trigram collocation detection.\n",
        "    if with_bigram:\n",
        "        bigram = gensim.models.Phrases(texts)\n",
        "        texts = [bigram[line] for line in texts]\n",
        "\n",
        "    if min_length:\n",
        "        texts = [[word for word in line if len(word)> min_length] for line in texts]\n",
        "\n",
        "    return texts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8slK66h6dKo",
        "colab_type": "text"
      },
      "source": [
        "###### 1) stopwords, bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4JrEuIp6cvg",
        "colab_type": "code",
        "outputId": "992c27fc-3b7e-4f35-d631-353a0bcaedcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "data_train = process_text(token_dataset_train, \n",
        "                          with_bigram=True)\n",
        "\n",
        "data_idx = [0,10,11,20,34,3456,6643]\n",
        "for idx in data_idx:\n",
        "  print('original:\\n', token_dataset_train[idx])\n",
        "  print('cleaned:\\n',data_train[idx])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            " ['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', '\\n', 'the', 'other', 'day', 'it', 'was', 'a', '2-door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', '\\n', 'early', '70s', 'it', 'was', 'called', 'a', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', '\\n', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', '\\n', 'all', 'i', 'know', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', 'engine', 'specs', 'years', '\\n', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', '\\n', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'e', 'mail']\n",
            "cleaned:\n",
            " ['wondering_anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sports_car', 'looked', 'late', 'early', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please_e', 'mail']\n",
            "original:\n",
            " ['i', 'have', 'a', 'line', 'on', 'a', 'ducati', '900gts', '1978', 'model', 'with', '17k', 'on', 'the', 'clock', ' ', 'runs', '\\n', 'very', 'well', 'paint', 'is', 'the', 'bronze', 'brown', 'orange', 'faded', 'out', 'leaks', 'a', 'bit', 'of', 'oil', '\\n', 'and', 'pops', 'out', 'of', '1st', 'with', 'hard', 'accel', ' ', 'the', 'shop', 'will', 'fix', 'trans', 'and', 'oil', '\\n', 'leak', ' ', 'they', 'sold', 'the', 'bike', 'to', 'the', '1', 'and', 'only', 'owner', ' ', 'they', 'want', '3495', 'and', '\\n', 'i', 'am', 'thinking', 'more', 'like', '3k.', ' ', 'any', 'opinions', 'out', 'there', ' ', 'please', 'email', 'me', '\\n', 'thanks', ' ', 'it', 'would', 'be', 'a', 'nice', 'stable', 'mate', 'to', 'the', 'beemer', ' ', 'then', 'i', \"'ll\", 'get', '\\n', 'a', 'jap', 'bike', 'and', 'call', 'myself', 'axis', 'motors', '\\n\\n', '--', '\\n', '-----------------------------------------------------------------------', '\\n', 'tuba', 'irwin', '     ', 'i', 'honk', 'therefore', 'i', 'am', '    ', 'computrac', 'richardson', 'tx', '\\n', 'irwin@cmptrc.lonestar.org', '   ', 'dod', '0826', '         ', 'r75/6']\n",
            "cleaned:\n",
            " ['line', 'ducati', 'model', 'clock', 'runs', 'well', 'paint', 'bronze', 'brown', 'orange', 'faded', 'leaks', 'bit', 'oil', 'pops', 'hard', 'accel', 'shop', 'fix', 'trans', 'oil', 'leak', 'sold', 'bike', 'owner', 'want', 'thinking', 'like', 'opinions', 'please_email', 'thanks', 'would_nice', 'stable', 'mate', 'beemer', 'get', 'jap', 'bike', 'call', 'axis', 'motors', 'tuba', 'irwin', 'honk', 'therefore', 'computrac', 'richardson', 'tx', 'dod']\n",
            "original:\n",
            " ['\\n', 'yep', 'that', \"'s\", 'pretty', 'much', 'it', 'i', \"'m\", 'not', 'a', 'jew', 'but', 'i', 'understand', 'that', 'this', 'is', 'the', '\\n', 'jewish', 'way', 'of', 'thinking', 'however', 'the', 'jews', 'believe', 'that', 'the', 'covenant', 'between', '\\n', 'yhwh', 'and', 'the', 'patriarchs', 'abraham', 'and', 'moses', 'in', 'this', 'case', 'establishes', 'a', 'moral', '\\n', 'code', 'to', 'follow', 'for', 'mankind', 'even', 'the', 'jews', 'could', 'not', 'decide', 'where', 'the', 'boundaries', '\\n', 'fall', 'though', '\\n\\n', 'as', 'i', 'understand', 'it', 'the', 'sadducees', 'believed', 'that', 'the', 'torah', 'was', 'all', 'that', 'was', '\\n', 'required', 'whereas', 'the', 'pharisees', 'the', 'ancestors', 'of', 'modern', 'judaism', 'believed', 'that', '\\n', 'the', 'torah', 'was', 'available', 'for', 'interpretation', 'to', 'lead', 'to', 'an', 'understanding', 'of', '\\n', 'the', 'required', 'morality', 'in', 'all', 'its', 'nuances', '->talmud', '\\n\\n', 'the', 'essence', 'of', 'all', 'of', 'this', 'is', 'that', 'biblical', 'morality', 'is', 'an', 'interface', 'between', '\\n', 'man', 'and', 'yhwh', 'for', 'a', 'jew', 'or', 'christian', 'and', 'does', 'not', 'necessarily', 'indicate', '\\n', 'anything', 'about', 'yhwh', 'outside', 'of', 'that', 'relationship', 'although', 'one', 'can', 'speculate', '\\n\\n\\n', 'the', 'trouble', 'with', 'all', 'of', 'this', 'is', 'that', 'we', 'do', \"n't\", 'really', 'know', 'what', 'the', 'created', '\\n', 'in', 'his', 'image', 'means', 'i', \"'ve\", 'heard', 'a', 'number', 'of', 'different', 'opinions', 'on', 'this', 'and', '\\n', 'have', 'still', 'not', 'come', 'to', 'any', 'conclusion', 'this', 'rather', 'upsets', 'the', 'apple', 'cart', 'if', '\\n', 'one', 'wants', 'to', 'base', 'a', 'life', 'script', 'on', 'this', 'shaky', 'foundation', 'to', 'mix', 'metaphors', '\\n', 'unashamedly', 'as', 'to', 'living', 'by', 'christ', \"'s\", 'example', 'we', 'know', 'very', 'little', 'about', '\\n', 'jesus', 'as', 'a', 'person', 'we', 'only', 'have', 'his', 'recorded', 'utterances', 'in', 'a', 'set', 'of', 'narratives', '\\n', 'by', 'his', 'followers', 'and', 'some', 'very', 'small', 'references', 'from', 'comtemporary', 'historians', '\\n', 'revelation', 'aside', 'one', 'can', 'only', 'know', 'christ', 'second', 'hand', 'or', 'worse', '\\n\\n', 'this', 'is', 'not', 'an', 'attempt', 'to', 'debunk', 'christianity', 'although', 'it', 'may', 'seem', 'that', 'way', '\\n', 'initially', 'the', 'point', 'i`m', 'trying', 'to', 'make', 'is', 'that', 'we', 'only', 'really', 'have', 'the', 'bible', '\\n', 'to', 'interpret', 'and', 'that', 'interpretation', 'is', 'by', 'humanity', 'i', 'guess', 'this', 'is', 'where', '\\n', 'faith', 'or', 'relevation', 'comes', 'in', 'with', 'all', 'its', 'inherent', 'subjectiveness', '\\n\\n\\n', 'no', 'there', 'may', 'be', 'an', 'absolute', 'moral', 'code', 'there', 'are', 'undoubtably', 'multiple', '\\n', 'moral', 'codes', 'the', 'multiple', 'moral', 'codes', 'may', 'be', 'founded', 'in', 'the', 'absolute', 'moral', '\\n', 'code', 'as', 'an', 'example', 'a', 'parent', 'may', 'tell', 'a', 'child', 'never', 'to', 'swear', 'and', 'the', 'child', '\\n', 'may', 'assume', 'that', 'the', 'parent', 'never', 'swears', 'simply', 'because', 'the', 'parent', 'has', 'told', '\\n', 'the', 'child', 'that', 'it', 'is', 'wrong', 'now', 'the', 'parent', 'may', 'swear', 'like', 'a', 'trooper', 'in', '\\n', 'the', 'pub', 'or', 'bar', 'where', 'there', 'are', 'no', 'children', 'the', 'wrongness', 'here', 'is', 'if', '\\n', 'the', 'child', 'disobeys', 'the', 'parent', 'the', 'parent', 'may', 'feel', 'that', 'it', 'is', 'inappropriate', '\\n', 'to', 'swear', 'in', 'front', 'of', 'children', 'but', 'may', 'be', 'quite', 'happy', 'to', 'swear', 'in', 'front', 'of', '\\n', 'animals', 'the', 'analogy', 'does', 'not', 'quite', 'hold', 'water', 'because', 'the', 'child', 'knows', 'that', '\\n', 'he', 'is', 'of', 'the', 'same', 'type', 'as', 'the', 'parent', 'and', 'may', 'be', 'a', 'parent', 'later', 'in', 'life', 'but', '\\n', 'you', 'get', 'the', 'gist', 'of', 'it', 'incidentally', 'the', 'young', 'child', 'considers', 'the', 'directive', '\\n', 'as', 'absolute', 'until', 'he', 'gets', 'older', 'see', 'piaget', 'and', 'learns', 'a', 'morality', 'of', 'his', 'own', '\\n\\n', 'david', '\\n\\n', '---', '\\n', 'on', 'religion']\n",
            "cleaned:\n",
            " ['yep', 'pretty_much', 'jew', 'understand', 'jewish', 'way', 'thinking', 'however', 'jews', 'believe', 'covenant', 'yhwh', 'patriarchs', 'abraham', 'moses', 'case', 'establishes', 'moral_code', 'follow', 'mankind', 'even', 'jews', 'could', 'decide', 'boundaries', 'fall', 'though', 'understand', 'sadducees', 'believed', 'torah', 'required', 'whereas', 'pharisees', 'ancestors', 'modern', 'judaism', 'believed', 'torah', 'available', 'interpretation', 'lead', 'understanding', 'required', 'morality', 'nuances', 'essence', 'biblical', 'morality', 'interface', 'man', 'yhwh', 'jew', 'christian', 'necessarily', 'indicate', 'anything', 'yhwh', 'outside', 'relationship', 'although', 'one', 'speculate', 'trouble', 'really', 'know', 'created', 'image', 'means', 'heard', 'number', 'different', 'opinions', 'still', 'come', 'conclusion', 'rather', 'upsets', 'apple', 'cart', 'one', 'wants', 'base', 'life', 'script', 'shaky', 'foundation', 'mix', 'metaphors', 'unashamedly', 'living', 'christ', 'example', 'know', 'little', 'jesus', 'person', 'recorded', 'utterances', 'set', 'narratives', 'followers', 'small', 'references', 'comtemporary', 'historians', 'revelation', 'aside', 'one', 'know', 'christ', 'second_hand', 'worse', 'attempt', 'debunk', 'christianity', 'although', 'may', 'seem', 'way', 'initially', 'point', 'trying', 'make', 'really', 'bible', 'interpret', 'interpretation', 'humanity', 'guess', 'faith', 'relevation', 'comes', 'inherent', 'subjectiveness', 'may', 'absolute_moral', 'code', 'undoubtably', 'multiple', 'moral', 'codes', 'multiple', 'moral', 'codes', 'may', 'founded', 'absolute_moral', 'code', 'example', 'parent', 'may', 'tell', 'child', 'never', 'swear', 'child', 'may', 'assume', 'parent', 'never', 'swears', 'simply', 'parent', 'told', 'child', 'wrong', 'parent', 'may', 'swear', 'like', 'trooper', 'pub', 'bar', 'children', 'wrongness', 'child', 'disobeys', 'parent', 'parent', 'may', 'feel', 'inappropriate', 'swear', 'front', 'children', 'may', 'quite_happy', 'swear', 'front', 'animals', 'analogy', 'quite', 'hold', 'water', 'child', 'knows', 'type', 'parent', 'may', 'parent', 'later', 'life', 'get', 'gist', 'incidentally', 'young', 'child', 'considers', 'directive', 'absolute', 'gets', 'older', 'see', 'piaget', 'learns', 'morality', 'david', 'religion']\n",
            "original:\n",
            " ['\\n', '...', '\\n\\n', 'these', 'do', \"n't\", 'seem', 'like', 'little', 'things', 'to', 'me', ' ', 'at', 'least', 'they', 'are', 'orders', '\\n', 'worse', 'than', 'the', 'motto', ' ', 'do', 'you', 'think', 'that', 'the', 'motto', 'is', 'a', 'little', 'thing', '\\n', 'that', 'will', 'lead', 'to', 'worse', 'things']\n",
            "cleaned:\n",
            " ['seem', 'like', 'little', 'things', 'least', 'orders', 'worse', 'motto', 'think', 'motto', 'little', 'thing', 'lead', 'worse', 'things']\n",
            "original:\n",
            " ['\\n', 'once', 'again', 'it', 'appears', 'that', 'the', 'one', 'eyed', 'man', 'has', 'appeared', 'in', 'the', 'land', 'of', 'the', 'sighted', '\\n', 'and', 'for', 'some', 'strange', 'resaon', 'has', 'appointed', 'himself', 'the', 'ruler', 'and', 'supreme', 'power']\n",
            "cleaned:\n",
            " ['appears', 'one', 'eyed', 'man', 'appeared', 'land', 'sighted', 'strange', 'resaon', 'appointed', 'ruler', 'supreme', 'power']\n",
            "original:\n",
            " ['\\n\\n\\n\\n\\n  ', 'actually', 'steve', 'i', 'think', 'he', 'was', 'refering', 'to', 'the', 'leafs', 'and', 'when', 'they', 'can', '\\n', 'be', 'expected', 'to', 'hit', 'the', 'greens', '...', '\\n']\n",
            "cleaned:\n",
            " ['actually', 'steve', 'think', 'refering', 'leafs', 'expected', 'hit', 'greens']\n",
            "original:\n",
            " ['does', 'anybody', 'else', 'think', 'that', 'ws', 'stats', 'should', 'become', 'part', 'of', '\\n', 'a', 'player', \"'s\", 'career', 'stats', '  ', 'why', 'not', '\\n \\n']\n",
            "cleaned:\n",
            " ['anybody_else', 'think', 'ws', 'stats', 'become', 'part', 'player', 'career', 'stats']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf3Kolk_6KA3",
        "colab_type": "text"
      },
      "source": [
        "###### 2) stopwords, lemmatize, bigram\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHtAkDGm20GZ",
        "colab_type": "code",
        "outputId": "7e55360c-6884-4380-b465-a9f228c964a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "data_train_lem = process_text(token_dataset_train, \n",
        "                          lemmatization = ['n'], #['a','n','v']\n",
        "                          with_bigram=True)\n",
        "\n",
        "data_idx = [0,10,11,20,34,3456,6643]\n",
        "for idx in data_idx:\n",
        "  print('original:\\n', token_dataset_train[idx])\n",
        "  print('cleaned:\\n',data_train_lem[idx])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            " ['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', '\\n', 'the', 'other', 'day', 'it', 'was', 'a', '2-door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', '\\n', 'early', '70s', 'it', 'was', 'called', 'a', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', '\\n', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', '\\n', 'all', 'i', 'know', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', 'engine', 'specs', 'years', '\\n', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', '\\n', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'e', 'mail']\n",
            "cleaned:\n",
            " ['wondering_anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sport_car', 'looked', 'late', 'early', 'called', 'bricklin', 'door', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please_e', 'mail']\n",
            "original:\n",
            " ['i', 'have', 'a', 'line', 'on', 'a', 'ducati', '900gts', '1978', 'model', 'with', '17k', 'on', 'the', 'clock', ' ', 'runs', '\\n', 'very', 'well', 'paint', 'is', 'the', 'bronze', 'brown', 'orange', 'faded', 'out', 'leaks', 'a', 'bit', 'of', 'oil', '\\n', 'and', 'pops', 'out', 'of', '1st', 'with', 'hard', 'accel', ' ', 'the', 'shop', 'will', 'fix', 'trans', 'and', 'oil', '\\n', 'leak', ' ', 'they', 'sold', 'the', 'bike', 'to', 'the', '1', 'and', 'only', 'owner', ' ', 'they', 'want', '3495', 'and', '\\n', 'i', 'am', 'thinking', 'more', 'like', '3k.', ' ', 'any', 'opinions', 'out', 'there', ' ', 'please', 'email', 'me', '\\n', 'thanks', ' ', 'it', 'would', 'be', 'a', 'nice', 'stable', 'mate', 'to', 'the', 'beemer', ' ', 'then', 'i', \"'ll\", 'get', '\\n', 'a', 'jap', 'bike', 'and', 'call', 'myself', 'axis', 'motors', '\\n\\n', '--', '\\n', '-----------------------------------------------------------------------', '\\n', 'tuba', 'irwin', '     ', 'i', 'honk', 'therefore', 'i', 'am', '    ', 'computrac', 'richardson', 'tx', '\\n', 'irwin@cmptrc.lonestar.org', '   ', 'dod', '0826', '         ', 'r75/6']\n",
            "cleaned:\n",
            " ['line', 'ducati', 'model', 'clock', 'run', 'well', 'paint', 'bronze', 'brown', 'orange', 'faded', 'leak', 'bit', 'oil', 'pop', 'hard', 'accel', 'shop', 'fix', 'trans', 'oil', 'leak', 'sold', 'bike', 'owner', 'want', 'thinking', 'like', 'opinion', 'please_email', 'thanks', 'would_nice', 'stable', 'mate', 'beemer', 'get', 'jap', 'bike', 'call', 'axis', 'motor', 'tuba', 'irwin', 'honk', 'therefore', 'computrac', 'richardson', 'tx', 'dod']\n",
            "original:\n",
            " ['\\n', 'yep', 'that', \"'s\", 'pretty', 'much', 'it', 'i', \"'m\", 'not', 'a', 'jew', 'but', 'i', 'understand', 'that', 'this', 'is', 'the', '\\n', 'jewish', 'way', 'of', 'thinking', 'however', 'the', 'jews', 'believe', 'that', 'the', 'covenant', 'between', '\\n', 'yhwh', 'and', 'the', 'patriarchs', 'abraham', 'and', 'moses', 'in', 'this', 'case', 'establishes', 'a', 'moral', '\\n', 'code', 'to', 'follow', 'for', 'mankind', 'even', 'the', 'jews', 'could', 'not', 'decide', 'where', 'the', 'boundaries', '\\n', 'fall', 'though', '\\n\\n', 'as', 'i', 'understand', 'it', 'the', 'sadducees', 'believed', 'that', 'the', 'torah', 'was', 'all', 'that', 'was', '\\n', 'required', 'whereas', 'the', 'pharisees', 'the', 'ancestors', 'of', 'modern', 'judaism', 'believed', 'that', '\\n', 'the', 'torah', 'was', 'available', 'for', 'interpretation', 'to', 'lead', 'to', 'an', 'understanding', 'of', '\\n', 'the', 'required', 'morality', 'in', 'all', 'its', 'nuances', '->talmud', '\\n\\n', 'the', 'essence', 'of', 'all', 'of', 'this', 'is', 'that', 'biblical', 'morality', 'is', 'an', 'interface', 'between', '\\n', 'man', 'and', 'yhwh', 'for', 'a', 'jew', 'or', 'christian', 'and', 'does', 'not', 'necessarily', 'indicate', '\\n', 'anything', 'about', 'yhwh', 'outside', 'of', 'that', 'relationship', 'although', 'one', 'can', 'speculate', '\\n\\n\\n', 'the', 'trouble', 'with', 'all', 'of', 'this', 'is', 'that', 'we', 'do', \"n't\", 'really', 'know', 'what', 'the', 'created', '\\n', 'in', 'his', 'image', 'means', 'i', \"'ve\", 'heard', 'a', 'number', 'of', 'different', 'opinions', 'on', 'this', 'and', '\\n', 'have', 'still', 'not', 'come', 'to', 'any', 'conclusion', 'this', 'rather', 'upsets', 'the', 'apple', 'cart', 'if', '\\n', 'one', 'wants', 'to', 'base', 'a', 'life', 'script', 'on', 'this', 'shaky', 'foundation', 'to', 'mix', 'metaphors', '\\n', 'unashamedly', 'as', 'to', 'living', 'by', 'christ', \"'s\", 'example', 'we', 'know', 'very', 'little', 'about', '\\n', 'jesus', 'as', 'a', 'person', 'we', 'only', 'have', 'his', 'recorded', 'utterances', 'in', 'a', 'set', 'of', 'narratives', '\\n', 'by', 'his', 'followers', 'and', 'some', 'very', 'small', 'references', 'from', 'comtemporary', 'historians', '\\n', 'revelation', 'aside', 'one', 'can', 'only', 'know', 'christ', 'second', 'hand', 'or', 'worse', '\\n\\n', 'this', 'is', 'not', 'an', 'attempt', 'to', 'debunk', 'christianity', 'although', 'it', 'may', 'seem', 'that', 'way', '\\n', 'initially', 'the', 'point', 'i`m', 'trying', 'to', 'make', 'is', 'that', 'we', 'only', 'really', 'have', 'the', 'bible', '\\n', 'to', 'interpret', 'and', 'that', 'interpretation', 'is', 'by', 'humanity', 'i', 'guess', 'this', 'is', 'where', '\\n', 'faith', 'or', 'relevation', 'comes', 'in', 'with', 'all', 'its', 'inherent', 'subjectiveness', '\\n\\n\\n', 'no', 'there', 'may', 'be', 'an', 'absolute', 'moral', 'code', 'there', 'are', 'undoubtably', 'multiple', '\\n', 'moral', 'codes', 'the', 'multiple', 'moral', 'codes', 'may', 'be', 'founded', 'in', 'the', 'absolute', 'moral', '\\n', 'code', 'as', 'an', 'example', 'a', 'parent', 'may', 'tell', 'a', 'child', 'never', 'to', 'swear', 'and', 'the', 'child', '\\n', 'may', 'assume', 'that', 'the', 'parent', 'never', 'swears', 'simply', 'because', 'the', 'parent', 'has', 'told', '\\n', 'the', 'child', 'that', 'it', 'is', 'wrong', 'now', 'the', 'parent', 'may', 'swear', 'like', 'a', 'trooper', 'in', '\\n', 'the', 'pub', 'or', 'bar', 'where', 'there', 'are', 'no', 'children', 'the', 'wrongness', 'here', 'is', 'if', '\\n', 'the', 'child', 'disobeys', 'the', 'parent', 'the', 'parent', 'may', 'feel', 'that', 'it', 'is', 'inappropriate', '\\n', 'to', 'swear', 'in', 'front', 'of', 'children', 'but', 'may', 'be', 'quite', 'happy', 'to', 'swear', 'in', 'front', 'of', '\\n', 'animals', 'the', 'analogy', 'does', 'not', 'quite', 'hold', 'water', 'because', 'the', 'child', 'knows', 'that', '\\n', 'he', 'is', 'of', 'the', 'same', 'type', 'as', 'the', 'parent', 'and', 'may', 'be', 'a', 'parent', 'later', 'in', 'life', 'but', '\\n', 'you', 'get', 'the', 'gist', 'of', 'it', 'incidentally', 'the', 'young', 'child', 'considers', 'the', 'directive', '\\n', 'as', 'absolute', 'until', 'he', 'gets', 'older', 'see', 'piaget', 'and', 'learns', 'a', 'morality', 'of', 'his', 'own', '\\n\\n', 'david', '\\n\\n', '---', '\\n', 'on', 'religion']\n",
            "cleaned:\n",
            " ['yep', 'pretty_much', 'jew', 'understand', 'jewish', 'way', 'thinking', 'however', 'jew', 'believe', 'covenant', 'yhwh', 'patriarch', 'abraham', 'moses', 'case', 'establishes', 'moral_code', 'follow', 'mankind', 'even', 'jew', 'could', 'decide', 'boundary', 'fall', 'though', 'understand', 'sadducee', 'believed', 'torah', 'required', 'whereas', 'pharisee', 'ancestor', 'modern', 'judaism', 'believed', 'torah', 'available', 'interpretation', 'lead', 'understanding', 'required', 'morality', 'nuance', 'essence', 'biblical', 'morality', 'interface', 'man', 'yhwh', 'jew', 'christian', 'necessarily', 'indicate', 'anything', 'yhwh', 'outside', 'relationship', 'although', 'one', 'speculate', 'trouble', 'really', 'know', 'created', 'image', 'mean', 'heard', 'number', 'different', 'opinion', 'still', 'come', 'conclusion', 'rather', 'upset', 'apple', 'cart', 'one', 'want', 'base', 'life', 'script', 'shaky', 'foundation', 'mix', 'metaphor', 'unashamedly', 'living', 'christ', 'example', 'know', 'little', 'jesus', 'person', 'recorded', 'utterance', 'set', 'narrative', 'follower', 'small', 'reference', 'comtemporary', 'historian', 'revelation', 'aside', 'one', 'know', 'christ', 'second_hand', 'worse', 'attempt', 'debunk', 'christianity', 'although', 'may', 'seem', 'way', 'initially', 'point', 'trying', 'make', 'really', 'bible', 'interpret', 'interpretation', 'humanity', 'guess', 'faith', 'relevation', 'come', 'inherent', 'subjectiveness', 'may', 'absolute_moral', 'code', 'undoubtably', 'multiple', 'moral_code', 'multiple', 'moral_code', 'may', 'founded', 'absolute_moral', 'code', 'example', 'parent', 'may', 'tell', 'child', 'never', 'swear', 'child', 'may', 'assume', 'parent', 'never', 'swears', 'simply', 'parent', 'told', 'child', 'wrong', 'parent', 'may', 'swear', 'like', 'trooper', 'pub', 'bar', 'child', 'wrongness', 'child', 'disobeys', 'parent', 'parent', 'may', 'feel', 'inappropriate', 'swear', 'front', 'child', 'may', 'quite_happy', 'swear', 'front', 'animal', 'analogy', 'quite', 'hold', 'water', 'child', 'know', 'type', 'parent', 'may', 'parent', 'later', 'life', 'get', 'gist', 'incidentally', 'young', 'child', 'considers', 'directive', 'absolute', 'get', 'older', 'see', 'piaget', 'learns', 'morality', 'david', 'religion']\n",
            "original:\n",
            " ['\\n', '...', '\\n\\n', 'these', 'do', \"n't\", 'seem', 'like', 'little', 'things', 'to', 'me', ' ', 'at', 'least', 'they', 'are', 'orders', '\\n', 'worse', 'than', 'the', 'motto', ' ', 'do', 'you', 'think', 'that', 'the', 'motto', 'is', 'a', 'little', 'thing', '\\n', 'that', 'will', 'lead', 'to', 'worse', 'things']\n",
            "cleaned:\n",
            " ['seem', 'like', 'little', 'thing', 'least', 'order', 'worse', 'motto', 'think', 'motto', 'little', 'thing', 'lead', 'worse', 'thing']\n",
            "original:\n",
            " ['\\n', 'once', 'again', 'it', 'appears', 'that', 'the', 'one', 'eyed', 'man', 'has', 'appeared', 'in', 'the', 'land', 'of', 'the', 'sighted', '\\n', 'and', 'for', 'some', 'strange', 'resaon', 'has', 'appointed', 'himself', 'the', 'ruler', 'and', 'supreme', 'power']\n",
            "cleaned:\n",
            " ['appears', 'one', 'eyed', 'man', 'appeared', 'land', 'sighted', 'strange', 'resaon', 'appointed', 'ruler', 'supreme', 'power']\n",
            "original:\n",
            " ['\\n\\n\\n\\n\\n  ', 'actually', 'steve', 'i', 'think', 'he', 'was', 'refering', 'to', 'the', 'leafs', 'and', 'when', 'they', 'can', '\\n', 'be', 'expected', 'to', 'hit', 'the', 'greens', '...', '\\n']\n",
            "cleaned:\n",
            " ['actually', 'steve', 'think', 'refering', 'leaf', 'expected', 'hit', 'green']\n",
            "original:\n",
            " ['does', 'anybody', 'else', 'think', 'that', 'ws', 'stats', 'should', 'become', 'part', 'of', '\\n', 'a', 'player', \"'s\", 'career', 'stats', '  ', 'why', 'not', '\\n \\n']\n",
            "cleaned:\n",
            " ['anybody_else', 'think', 'w', 'stats', 'become', 'part', 'player', 'career', 'stats']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnAdOas-CS0w",
        "colab_type": "code",
        "outputId": "95f11b0e-e631-44c8-a28f-89391f7818ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "#also lemmatize a v\n",
        "data_train_lem = process_text(token_dataset_train, \n",
        "                          lemmatization = ['a','n','v'],\n",
        "                          with_bigram=True)\n",
        "\n",
        "data_idx = [0,10,11,20,34,3456,6643]\n",
        "for idx in data_idx:\n",
        "  print('original:\\n', token_dataset_train[idx])\n",
        "  print('cleaned:\\n',data_train_lem[idx])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            " ['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', '\\n', 'the', 'other', 'day', 'it', 'was', 'a', '2-door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', '\\n', 'early', '70s', 'it', 'was', 'called', 'a', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', '\\n', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', '\\n', 'all', 'i', 'know', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', 'engine', 'specs', 'years', '\\n', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', '\\n', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'e', 'mail']\n",
            "cleaned:\n",
            " ['wonder_anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sport_car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'whatever', 'info', 'funky', 'look', 'car', 'please_e', 'mail']\n",
            "original:\n",
            " ['i', 'have', 'a', 'line', 'on', 'a', 'ducati', '900gts', '1978', 'model', 'with', '17k', 'on', 'the', 'clock', ' ', 'runs', '\\n', 'very', 'well', 'paint', 'is', 'the', 'bronze', 'brown', 'orange', 'faded', 'out', 'leaks', 'a', 'bit', 'of', 'oil', '\\n', 'and', 'pops', 'out', 'of', '1st', 'with', 'hard', 'accel', ' ', 'the', 'shop', 'will', 'fix', 'trans', 'and', 'oil', '\\n', 'leak', ' ', 'they', 'sold', 'the', 'bike', 'to', 'the', '1', 'and', 'only', 'owner', ' ', 'they', 'want', '3495', 'and', '\\n', 'i', 'am', 'thinking', 'more', 'like', '3k.', ' ', 'any', 'opinions', 'out', 'there', ' ', 'please', 'email', 'me', '\\n', 'thanks', ' ', 'it', 'would', 'be', 'a', 'nice', 'stable', 'mate', 'to', 'the', 'beemer', ' ', 'then', 'i', \"'ll\", 'get', '\\n', 'a', 'jap', 'bike', 'and', 'call', 'myself', 'axis', 'motors', '\\n\\n', '--', '\\n', '-----------------------------------------------------------------------', '\\n', 'tuba', 'irwin', '     ', 'i', 'honk', 'therefore', 'i', 'am', '    ', 'computrac', 'richardson', 'tx', '\\n', 'irwin@cmptrc.lonestar.org', '   ', 'dod', '0826', '         ', 'r75/6']\n",
            "cleaned:\n",
            " ['line', 'ducati', 'model', 'clock', 'run', 'well', 'paint', 'bronze', 'brown', 'orange', 'fade', 'leak', 'bit', 'oil', 'pop', 'hard', 'accel', 'shop', 'fix', 'trans', 'oil', 'leak', 'sell', 'bike', 'owner', 'want', 'think', 'like', 'opinion', 'please_email', 'thanks', 'would_nice', 'stable', 'mate', 'beemer', 'get', 'jap', 'bike', 'call', 'axis', 'motor', 'tuba', 'irwin', 'honk', 'therefore', 'computrac', 'richardson', 'tx', 'dod']\n",
            "original:\n",
            " ['\\n', 'yep', 'that', \"'s\", 'pretty', 'much', 'it', 'i', \"'m\", 'not', 'a', 'jew', 'but', 'i', 'understand', 'that', 'this', 'is', 'the', '\\n', 'jewish', 'way', 'of', 'thinking', 'however', 'the', 'jews', 'believe', 'that', 'the', 'covenant', 'between', '\\n', 'yhwh', 'and', 'the', 'patriarchs', 'abraham', 'and', 'moses', 'in', 'this', 'case', 'establishes', 'a', 'moral', '\\n', 'code', 'to', 'follow', 'for', 'mankind', 'even', 'the', 'jews', 'could', 'not', 'decide', 'where', 'the', 'boundaries', '\\n', 'fall', 'though', '\\n\\n', 'as', 'i', 'understand', 'it', 'the', 'sadducees', 'believed', 'that', 'the', 'torah', 'was', 'all', 'that', 'was', '\\n', 'required', 'whereas', 'the', 'pharisees', 'the', 'ancestors', 'of', 'modern', 'judaism', 'believed', 'that', '\\n', 'the', 'torah', 'was', 'available', 'for', 'interpretation', 'to', 'lead', 'to', 'an', 'understanding', 'of', '\\n', 'the', 'required', 'morality', 'in', 'all', 'its', 'nuances', '->talmud', '\\n\\n', 'the', 'essence', 'of', 'all', 'of', 'this', 'is', 'that', 'biblical', 'morality', 'is', 'an', 'interface', 'between', '\\n', 'man', 'and', 'yhwh', 'for', 'a', 'jew', 'or', 'christian', 'and', 'does', 'not', 'necessarily', 'indicate', '\\n', 'anything', 'about', 'yhwh', 'outside', 'of', 'that', 'relationship', 'although', 'one', 'can', 'speculate', '\\n\\n\\n', 'the', 'trouble', 'with', 'all', 'of', 'this', 'is', 'that', 'we', 'do', \"n't\", 'really', 'know', 'what', 'the', 'created', '\\n', 'in', 'his', 'image', 'means', 'i', \"'ve\", 'heard', 'a', 'number', 'of', 'different', 'opinions', 'on', 'this', 'and', '\\n', 'have', 'still', 'not', 'come', 'to', 'any', 'conclusion', 'this', 'rather', 'upsets', 'the', 'apple', 'cart', 'if', '\\n', 'one', 'wants', 'to', 'base', 'a', 'life', 'script', 'on', 'this', 'shaky', 'foundation', 'to', 'mix', 'metaphors', '\\n', 'unashamedly', 'as', 'to', 'living', 'by', 'christ', \"'s\", 'example', 'we', 'know', 'very', 'little', 'about', '\\n', 'jesus', 'as', 'a', 'person', 'we', 'only', 'have', 'his', 'recorded', 'utterances', 'in', 'a', 'set', 'of', 'narratives', '\\n', 'by', 'his', 'followers', 'and', 'some', 'very', 'small', 'references', 'from', 'comtemporary', 'historians', '\\n', 'revelation', 'aside', 'one', 'can', 'only', 'know', 'christ', 'second', 'hand', 'or', 'worse', '\\n\\n', 'this', 'is', 'not', 'an', 'attempt', 'to', 'debunk', 'christianity', 'although', 'it', 'may', 'seem', 'that', 'way', '\\n', 'initially', 'the', 'point', 'i`m', 'trying', 'to', 'make', 'is', 'that', 'we', 'only', 'really', 'have', 'the', 'bible', '\\n', 'to', 'interpret', 'and', 'that', 'interpretation', 'is', 'by', 'humanity', 'i', 'guess', 'this', 'is', 'where', '\\n', 'faith', 'or', 'relevation', 'comes', 'in', 'with', 'all', 'its', 'inherent', 'subjectiveness', '\\n\\n\\n', 'no', 'there', 'may', 'be', 'an', 'absolute', 'moral', 'code', 'there', 'are', 'undoubtably', 'multiple', '\\n', 'moral', 'codes', 'the', 'multiple', 'moral', 'codes', 'may', 'be', 'founded', 'in', 'the', 'absolute', 'moral', '\\n', 'code', 'as', 'an', 'example', 'a', 'parent', 'may', 'tell', 'a', 'child', 'never', 'to', 'swear', 'and', 'the', 'child', '\\n', 'may', 'assume', 'that', 'the', 'parent', 'never', 'swears', 'simply', 'because', 'the', 'parent', 'has', 'told', '\\n', 'the', 'child', 'that', 'it', 'is', 'wrong', 'now', 'the', 'parent', 'may', 'swear', 'like', 'a', 'trooper', 'in', '\\n', 'the', 'pub', 'or', 'bar', 'where', 'there', 'are', 'no', 'children', 'the', 'wrongness', 'here', 'is', 'if', '\\n', 'the', 'child', 'disobeys', 'the', 'parent', 'the', 'parent', 'may', 'feel', 'that', 'it', 'is', 'inappropriate', '\\n', 'to', 'swear', 'in', 'front', 'of', 'children', 'but', 'may', 'be', 'quite', 'happy', 'to', 'swear', 'in', 'front', 'of', '\\n', 'animals', 'the', 'analogy', 'does', 'not', 'quite', 'hold', 'water', 'because', 'the', 'child', 'knows', 'that', '\\n', 'he', 'is', 'of', 'the', 'same', 'type', 'as', 'the', 'parent', 'and', 'may', 'be', 'a', 'parent', 'later', 'in', 'life', 'but', '\\n', 'you', 'get', 'the', 'gist', 'of', 'it', 'incidentally', 'the', 'young', 'child', 'considers', 'the', 'directive', '\\n', 'as', 'absolute', 'until', 'he', 'gets', 'older', 'see', 'piaget', 'and', 'learns', 'a', 'morality', 'of', 'his', 'own', '\\n\\n', 'david', '\\n\\n', '---', '\\n', 'on', 'religion']\n",
            "cleaned:\n",
            " ['yep', 'pretty_much', 'jew', 'understand', 'jewish', 'way', 'think', 'however', 'jew', 'believe', 'covenant', 'yhwh', 'patriarch', 'abraham', 'moses', 'case', 'establish', 'moral_code', 'follow', 'mankind', 'even', 'jew', 'could', 'decide', 'boundary', 'fall', 'though', 'understand', 'sadducee', 'believe', 'torah', 'require', 'whereas', 'pharisee', 'ancestor', 'modern', 'judaism', 'believe', 'torah', 'available', 'interpretation', 'lead', 'understanding', 'require', 'morality', 'nuance', 'essence', 'biblical', 'morality', 'interface', 'man', 'yhwh', 'jew', 'christian', 'necessarily', 'indicate', 'anything', 'yhwh', 'outside', 'relationship', 'although', 'one', 'speculate', 'trouble', 'really', 'know', 'create', 'image', 'mean', 'heard', 'number', 'different', 'opinion', 'still', 'come_conclusion', 'rather', 'upset', 'apple', 'cart', 'one', 'want', 'base', 'life', 'script', 'shaky', 'foundation', 'mix', 'metaphor', 'unashamedly', 'live', 'christ', 'example', 'know', 'little', 'jesus', 'person', 'record', 'utterance', 'set', 'narrative', 'follower', 'small', 'reference', 'comtemporary', 'historian', 'revelation', 'aside', 'one', 'know', 'christ', 'second', 'hand', 'worse', 'attempt', 'debunk', 'christianity', 'although', 'may', 'seem', 'way', 'initially', 'point', 'try', 'make', 'really', 'bible', 'interpret', 'interpretation', 'humanity', 'guess', 'faith', 'relevation', 'come', 'inherent', 'subjectiveness', 'may', 'absolute_moral', 'code', 'undoubtably', 'multiple', 'moral_code', 'multiple', 'moral_code', 'may', 'found', 'absolute_moral', 'code', 'example', 'parent', 'may', 'tell', 'child', 'never', 'swear', 'child', 'may', 'assume', 'parent', 'never', 'swear', 'simply', 'parent', 'tell', 'child', 'wrong', 'parent', 'may', 'swear', 'like', 'trooper', 'pub', 'bar', 'child', 'wrongness', 'child', 'disobeys', 'parent', 'parent', 'may', 'feel', 'inappropriate', 'swear', 'front', 'child', 'may', 'quite_happy', 'swear', 'front', 'animal', 'analogy', 'quite', 'hold', 'water', 'child', 'know', 'type', 'parent', 'may', 'parent', 'later', 'life', 'get', 'gist', 'incidentally', 'young', 'child', 'considers', 'directive', 'absolute', 'get', 'older', 'see', 'piaget', 'learn', 'morality', 'david', 'religion']\n",
            "original:\n",
            " ['\\n', '...', '\\n\\n', 'these', 'do', \"n't\", 'seem', 'like', 'little', 'things', 'to', 'me', ' ', 'at', 'least', 'they', 'are', 'orders', '\\n', 'worse', 'than', 'the', 'motto', ' ', 'do', 'you', 'think', 'that', 'the', 'motto', 'is', 'a', 'little', 'thing', '\\n', 'that', 'will', 'lead', 'to', 'worse', 'things']\n",
            "cleaned:\n",
            " ['seem', 'like', 'little', 'thing', 'least', 'order', 'worse', 'motto', 'think_motto', 'little', 'thing', 'lead', 'worse', 'thing']\n",
            "original:\n",
            " ['\\n', 'once', 'again', 'it', 'appears', 'that', 'the', 'one', 'eyed', 'man', 'has', 'appeared', 'in', 'the', 'land', 'of', 'the', 'sighted', '\\n', 'and', 'for', 'some', 'strange', 'resaon', 'has', 'appointed', 'himself', 'the', 'ruler', 'and', 'supreme', 'power']\n",
            "cleaned:\n",
            " ['appear', 'one', 'eyed', 'man', 'appear', 'land', 'sight', 'strange', 'resaon', 'appoint', 'ruler', 'supreme', 'power']\n",
            "original:\n",
            " ['\\n\\n\\n\\n\\n  ', 'actually', 'steve', 'i', 'think', 'he', 'was', 'refering', 'to', 'the', 'leafs', 'and', 'when', 'they', 'can', '\\n', 'be', 'expected', 'to', 'hit', 'the', 'greens', '...', '\\n']\n",
            "cleaned:\n",
            " ['actually', 'steve', 'think', 'refer', 'leaf', 'expect', 'hit', 'green']\n",
            "original:\n",
            " ['does', 'anybody', 'else', 'think', 'that', 'ws', 'stats', 'should', 'become', 'part', 'of', '\\n', 'a', 'player', \"'s\", 'career', 'stats', '  ', 'why', 'not', '\\n \\n']\n",
            "cleaned:\n",
            " ['anybody_else', 'think', 'w', 'stats', 'become', 'part', 'player', 'career', 'stats']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY15Bgy36ReO",
        "colab_type": "text"
      },
      "source": [
        "###### 3) stopwords, lemmatize, bigram, min_length > 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGGpxLJQ-ctA",
        "colab_type": "code",
        "outputId": "7a55c842-c107-49d7-8be9-6c7ade6d40d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "#Bigran and remove nltk stopwords\n",
        "data_train_lem_2 = process_text(token_dataset_train, \n",
        "                          lemmatization = ['a','n','v'],\n",
        "                          with_bigram=True,\n",
        "                          min_length = 1)\n",
        "\n",
        "data_idx = [0,10,11,20,34,3456,6643]\n",
        "for idx in data_idx:\n",
        "  print('original:\\n', token_dataset_train[idx])\n",
        "  print('cleaned:\\n',data_train_lem_2[idx])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            " ['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', '\\n', 'the', 'other', 'day', 'it', 'was', 'a', '2-door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', '\\n', 'early', '70s', 'it', 'was', 'called', 'a', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', '\\n', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', '\\n', 'all', 'i', 'know', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', 'engine', 'specs', 'years', '\\n', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', '\\n', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'e', 'mail']\n",
            "cleaned:\n",
            " ['wonder_anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sport_car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'whatever', 'info', 'funky', 'look', 'car', 'please_e', 'mail']\n",
            "original:\n",
            " ['i', 'have', 'a', 'line', 'on', 'a', 'ducati', '900gts', '1978', 'model', 'with', '17k', 'on', 'the', 'clock', ' ', 'runs', '\\n', 'very', 'well', 'paint', 'is', 'the', 'bronze', 'brown', 'orange', 'faded', 'out', 'leaks', 'a', 'bit', 'of', 'oil', '\\n', 'and', 'pops', 'out', 'of', '1st', 'with', 'hard', 'accel', ' ', 'the', 'shop', 'will', 'fix', 'trans', 'and', 'oil', '\\n', 'leak', ' ', 'they', 'sold', 'the', 'bike', 'to', 'the', '1', 'and', 'only', 'owner', ' ', 'they', 'want', '3495', 'and', '\\n', 'i', 'am', 'thinking', 'more', 'like', '3k.', ' ', 'any', 'opinions', 'out', 'there', ' ', 'please', 'email', 'me', '\\n', 'thanks', ' ', 'it', 'would', 'be', 'a', 'nice', 'stable', 'mate', 'to', 'the', 'beemer', ' ', 'then', 'i', \"'ll\", 'get', '\\n', 'a', 'jap', 'bike', 'and', 'call', 'myself', 'axis', 'motors', '\\n\\n', '--', '\\n', '-----------------------------------------------------------------------', '\\n', 'tuba', 'irwin', '     ', 'i', 'honk', 'therefore', 'i', 'am', '    ', 'computrac', 'richardson', 'tx', '\\n', 'irwin@cmptrc.lonestar.org', '   ', 'dod', '0826', '         ', 'r75/6']\n",
            "cleaned:\n",
            " ['line', 'ducati', 'model', 'clock', 'run', 'well', 'paint', 'bronze', 'brown', 'orange', 'fade', 'leak', 'bit', 'oil', 'pop', 'hard', 'accel', 'shop', 'fix', 'trans', 'oil', 'leak', 'sell', 'bike', 'owner', 'want', 'think', 'like', 'opinion', 'please_email', 'thanks', 'would_nice', 'stable', 'mate', 'beemer', 'get', 'jap', 'bike', 'call', 'axis', 'motor', 'tuba', 'irwin', 'honk', 'therefore', 'computrac', 'richardson', 'tx', 'dod']\n",
            "original:\n",
            " ['\\n', 'yep', 'that', \"'s\", 'pretty', 'much', 'it', 'i', \"'m\", 'not', 'a', 'jew', 'but', 'i', 'understand', 'that', 'this', 'is', 'the', '\\n', 'jewish', 'way', 'of', 'thinking', 'however', 'the', 'jews', 'believe', 'that', 'the', 'covenant', 'between', '\\n', 'yhwh', 'and', 'the', 'patriarchs', 'abraham', 'and', 'moses', 'in', 'this', 'case', 'establishes', 'a', 'moral', '\\n', 'code', 'to', 'follow', 'for', 'mankind', 'even', 'the', 'jews', 'could', 'not', 'decide', 'where', 'the', 'boundaries', '\\n', 'fall', 'though', '\\n\\n', 'as', 'i', 'understand', 'it', 'the', 'sadducees', 'believed', 'that', 'the', 'torah', 'was', 'all', 'that', 'was', '\\n', 'required', 'whereas', 'the', 'pharisees', 'the', 'ancestors', 'of', 'modern', 'judaism', 'believed', 'that', '\\n', 'the', 'torah', 'was', 'available', 'for', 'interpretation', 'to', 'lead', 'to', 'an', 'understanding', 'of', '\\n', 'the', 'required', 'morality', 'in', 'all', 'its', 'nuances', '->talmud', '\\n\\n', 'the', 'essence', 'of', 'all', 'of', 'this', 'is', 'that', 'biblical', 'morality', 'is', 'an', 'interface', 'between', '\\n', 'man', 'and', 'yhwh', 'for', 'a', 'jew', 'or', 'christian', 'and', 'does', 'not', 'necessarily', 'indicate', '\\n', 'anything', 'about', 'yhwh', 'outside', 'of', 'that', 'relationship', 'although', 'one', 'can', 'speculate', '\\n\\n\\n', 'the', 'trouble', 'with', 'all', 'of', 'this', 'is', 'that', 'we', 'do', \"n't\", 'really', 'know', 'what', 'the', 'created', '\\n', 'in', 'his', 'image', 'means', 'i', \"'ve\", 'heard', 'a', 'number', 'of', 'different', 'opinions', 'on', 'this', 'and', '\\n', 'have', 'still', 'not', 'come', 'to', 'any', 'conclusion', 'this', 'rather', 'upsets', 'the', 'apple', 'cart', 'if', '\\n', 'one', 'wants', 'to', 'base', 'a', 'life', 'script', 'on', 'this', 'shaky', 'foundation', 'to', 'mix', 'metaphors', '\\n', 'unashamedly', 'as', 'to', 'living', 'by', 'christ', \"'s\", 'example', 'we', 'know', 'very', 'little', 'about', '\\n', 'jesus', 'as', 'a', 'person', 'we', 'only', 'have', 'his', 'recorded', 'utterances', 'in', 'a', 'set', 'of', 'narratives', '\\n', 'by', 'his', 'followers', 'and', 'some', 'very', 'small', 'references', 'from', 'comtemporary', 'historians', '\\n', 'revelation', 'aside', 'one', 'can', 'only', 'know', 'christ', 'second', 'hand', 'or', 'worse', '\\n\\n', 'this', 'is', 'not', 'an', 'attempt', 'to', 'debunk', 'christianity', 'although', 'it', 'may', 'seem', 'that', 'way', '\\n', 'initially', 'the', 'point', 'i`m', 'trying', 'to', 'make', 'is', 'that', 'we', 'only', 'really', 'have', 'the', 'bible', '\\n', 'to', 'interpret', 'and', 'that', 'interpretation', 'is', 'by', 'humanity', 'i', 'guess', 'this', 'is', 'where', '\\n', 'faith', 'or', 'relevation', 'comes', 'in', 'with', 'all', 'its', 'inherent', 'subjectiveness', '\\n\\n\\n', 'no', 'there', 'may', 'be', 'an', 'absolute', 'moral', 'code', 'there', 'are', 'undoubtably', 'multiple', '\\n', 'moral', 'codes', 'the', 'multiple', 'moral', 'codes', 'may', 'be', 'founded', 'in', 'the', 'absolute', 'moral', '\\n', 'code', 'as', 'an', 'example', 'a', 'parent', 'may', 'tell', 'a', 'child', 'never', 'to', 'swear', 'and', 'the', 'child', '\\n', 'may', 'assume', 'that', 'the', 'parent', 'never', 'swears', 'simply', 'because', 'the', 'parent', 'has', 'told', '\\n', 'the', 'child', 'that', 'it', 'is', 'wrong', 'now', 'the', 'parent', 'may', 'swear', 'like', 'a', 'trooper', 'in', '\\n', 'the', 'pub', 'or', 'bar', 'where', 'there', 'are', 'no', 'children', 'the', 'wrongness', 'here', 'is', 'if', '\\n', 'the', 'child', 'disobeys', 'the', 'parent', 'the', 'parent', 'may', 'feel', 'that', 'it', 'is', 'inappropriate', '\\n', 'to', 'swear', 'in', 'front', 'of', 'children', 'but', 'may', 'be', 'quite', 'happy', 'to', 'swear', 'in', 'front', 'of', '\\n', 'animals', 'the', 'analogy', 'does', 'not', 'quite', 'hold', 'water', 'because', 'the', 'child', 'knows', 'that', '\\n', 'he', 'is', 'of', 'the', 'same', 'type', 'as', 'the', 'parent', 'and', 'may', 'be', 'a', 'parent', 'later', 'in', 'life', 'but', '\\n', 'you', 'get', 'the', 'gist', 'of', 'it', 'incidentally', 'the', 'young', 'child', 'considers', 'the', 'directive', '\\n', 'as', 'absolute', 'until', 'he', 'gets', 'older', 'see', 'piaget', 'and', 'learns', 'a', 'morality', 'of', 'his', 'own', '\\n\\n', 'david', '\\n\\n', '---', '\\n', 'on', 'religion']\n",
            "cleaned:\n",
            " ['yep', 'pretty_much', 'jew', 'understand', 'jewish', 'way', 'think', 'however', 'jew', 'believe', 'covenant', 'yhwh', 'patriarch', 'abraham', 'moses', 'case', 'establish', 'moral_code', 'follow', 'mankind', 'even', 'jew', 'could', 'decide', 'boundary', 'fall', 'though', 'understand', 'sadducee', 'believe', 'torah', 'require', 'whereas', 'pharisee', 'ancestor', 'modern', 'judaism', 'believe', 'torah', 'available', 'interpretation', 'lead', 'understanding', 'require', 'morality', 'nuance', 'essence', 'biblical', 'morality', 'interface', 'man', 'yhwh', 'jew', 'christian', 'necessarily', 'indicate', 'anything', 'yhwh', 'outside', 'relationship', 'although', 'one', 'speculate', 'trouble', 'really', 'know', 'create', 'image', 'mean', 'heard', 'number', 'different', 'opinion', 'still', 'come_conclusion', 'rather', 'upset', 'apple', 'cart', 'one', 'want', 'base', 'life', 'script', 'shaky', 'foundation', 'mix', 'metaphor', 'unashamedly', 'live', 'christ', 'example', 'know', 'little', 'jesus', 'person', 'record', 'utterance', 'set', 'narrative', 'follower', 'small', 'reference', 'comtemporary', 'historian', 'revelation', 'aside', 'one', 'know', 'christ', 'second', 'hand', 'worse', 'attempt', 'debunk', 'christianity', 'although', 'may', 'seem', 'way', 'initially', 'point', 'try', 'make', 'really', 'bible', 'interpret', 'interpretation', 'humanity', 'guess', 'faith', 'relevation', 'come', 'inherent', 'subjectiveness', 'may', 'absolute_moral', 'code', 'undoubtably', 'multiple', 'moral_code', 'multiple', 'moral_code', 'may', 'found', 'absolute_moral', 'code', 'example', 'parent', 'may', 'tell', 'child', 'never', 'swear', 'child', 'may', 'assume', 'parent', 'never', 'swear', 'simply', 'parent', 'tell', 'child', 'wrong', 'parent', 'may', 'swear', 'like', 'trooper', 'pub', 'bar', 'child', 'wrongness', 'child', 'disobeys', 'parent', 'parent', 'may', 'feel', 'inappropriate', 'swear', 'front', 'child', 'may', 'quite_happy', 'swear', 'front', 'animal', 'analogy', 'quite', 'hold', 'water', 'child', 'know', 'type', 'parent', 'may', 'parent', 'later', 'life', 'get', 'gist', 'incidentally', 'young', 'child', 'considers', 'directive', 'absolute', 'get', 'older', 'see', 'piaget', 'learn', 'morality', 'david', 'religion']\n",
            "original:\n",
            " ['\\n', '...', '\\n\\n', 'these', 'do', \"n't\", 'seem', 'like', 'little', 'things', 'to', 'me', ' ', 'at', 'least', 'they', 'are', 'orders', '\\n', 'worse', 'than', 'the', 'motto', ' ', 'do', 'you', 'think', 'that', 'the', 'motto', 'is', 'a', 'little', 'thing', '\\n', 'that', 'will', 'lead', 'to', 'worse', 'things']\n",
            "cleaned:\n",
            " ['seem', 'like', 'little', 'thing', 'least', 'order', 'worse', 'motto', 'think_motto', 'little', 'thing', 'lead', 'worse', 'thing']\n",
            "original:\n",
            " ['\\n', 'once', 'again', 'it', 'appears', 'that', 'the', 'one', 'eyed', 'man', 'has', 'appeared', 'in', 'the', 'land', 'of', 'the', 'sighted', '\\n', 'and', 'for', 'some', 'strange', 'resaon', 'has', 'appointed', 'himself', 'the', 'ruler', 'and', 'supreme', 'power']\n",
            "cleaned:\n",
            " ['appear', 'one', 'eyed', 'man', 'appear', 'land', 'sight', 'strange', 'resaon', 'appoint', 'ruler', 'supreme', 'power']\n",
            "original:\n",
            " ['\\n\\n\\n\\n\\n  ', 'actually', 'steve', 'i', 'think', 'he', 'was', 'refering', 'to', 'the', 'leafs', 'and', 'when', 'they', 'can', '\\n', 'be', 'expected', 'to', 'hit', 'the', 'greens', '...', '\\n']\n",
            "cleaned:\n",
            " ['actually', 'steve', 'think', 'refer', 'leaf', 'expect', 'hit', 'green']\n",
            "original:\n",
            " ['does', 'anybody', 'else', 'think', 'that', 'ws', 'stats', 'should', 'become', 'part', 'of', '\\n', 'a', 'player', \"'s\", 'career', 'stats', '  ', 'why', 'not', '\\n \\n']\n",
            "cleaned:\n",
            " ['anybody_else', 'think', 'stats', 'become', 'part', 'player', 'career', 'stats']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VU0Eht98zB-",
        "colab_type": "text"
      },
      "source": [
        "###### save cleaned tokens dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCUvkJxNp0AV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pkl.dump(data_train, open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_train_bi.p','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tkkJBQJ9mfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#stopwords, lemmatize a n v, bigram, min_length > 1\n",
        "pkl.dump(data_train_lem_2, open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_train_lem_2.p','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik6MAknK_5A_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#stopwords, bigram\n",
        "pkl.dump(data_train, open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_train_main_bi.p','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXEB4e5u90tc",
        "colab_type": "text"
      },
      "source": [
        "###### same process for test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJq3TphLpu2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Test Set\n",
        "token_dataset_test = pkl.load(open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_test.p\", \"rb\"))\n",
        "all_tokens_test = pkl.load(open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/all_tokens_20newsgroups_test.p\", \"rb\"))\n",
        "\n",
        "data_test = process_text(token_dataset_test, \n",
        "                          with_bigram=True,\n",
        "                          filter_stopword_spacy = False,\n",
        "                          remove_non_eng = False,\n",
        "                          with_lemmatize = False, \n",
        "                          with_stem = False)\n",
        "\n",
        "pkl.dump(data_test, open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_test_bi.p','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJVxcZkF9_EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test set without headers, footers, quotes\n",
        "## Test Set stopwords, lemmatize a n v, bigram, min_length > 1\n",
        "token_dataset_test = pkl.load(open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_maintext_test.p\", \"rb\"))\n",
        "all_tokens_test = pkl.load(open(\"/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/all_tokens_20newsgroups_maintext_test.p\", \"rb\"))\n",
        "\n",
        "data_test = process_text(token_dataset_test, \n",
        "                          with_bigram=True)\n",
        "\n",
        "pkl.dump(data_test, open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_test_main_bi.p','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oicv6aa_1-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMqD8HSgpk95",
        "colab_type": "text"
      },
      "source": [
        "#### Load Preprocessed Clean Tokenized Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGBIAWHa6INg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with header...\n",
        "# data_train = pkl.load(open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_train_bi.p','rb'))\n",
        "# data_test = pkl.load(open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_test_bi.p','rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-gOiuFJEcri",
        "colab_type": "code",
        "outputId": "18e97637-cf3c-41c9-99b4-00300585cdd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#without headers\n",
        "# lem a n v min_length >1 \n",
        "data_train = pkl.load(open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_train_lem_2.p','rb'))\n",
        "data_test = pkl.load(open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_test_lem_2.p','rb'))\n",
        "print(data_train[0])\n",
        "print(data_test[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['wonder_anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sport_car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'whatever', 'info', 'funky', 'look', 'car', 'please_e', 'mail']\n",
            "['little', 'confused', 'model', 'bonnevilles', 'heard', 'le', 'se', 'lse', 'sse', 'ssei', 'could_someone', 'tell_difference', 'far', 'feature', 'performance', 'also', 'curious_know', 'book', 'value', 'prefereably', 'model', 'much_le', 'book', 'value', 'usually', 'get', 'word', 'much', 'demand', 'time', 'year', 'heard', 'mid', 'spring', 'early', 'summer', 'best', 'time', 'buy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOPBLSv1ATYp",
        "colab_type": "code",
        "outputId": "3669e6a0-06c1-4e7a-c514-0212aa613794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#without headers\n",
        "data_train = pkl.load(open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_train_main_bi.p','rb'))\n",
        "data_test = pkl.load(open('/content/drive/My Drive/GradAcademics/2019Fall/Capstone/data/tokenized_20newsgroups_test_main_bi.p','rb'))\n",
        "print(data_train[0])\n",
        "print(data_test[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['wondering_anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sports_car', 'looked', 'late', 'early', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please_e', 'mail']\n",
            "['little', 'confused', 'models', 'bonnevilles', 'heard', 'le', 'se', 'lse', 'sse', 'ssei', 'could_someone', 'tell', 'differences', 'far', 'features', 'performance', 'also', 'curious', 'know', 'book', 'value', 'prefereably', 'model', 'much_less', 'book', 'value', 'usually', 'get', 'words', 'much', 'demand', 'time', 'year', 'heard', 'mid', 'spring', 'early', 'summer', 'best', 'time', 'buy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b64LuVSI6INl",
        "colab_type": "text"
      },
      "source": [
        "#### vocabulary prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0Lzxtjhm8eY",
        "colab_type": "text"
      },
      "source": [
        "##### vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKkkAl18xfY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(data_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AksO1TjNxpUH",
        "colab_type": "code",
        "outputId": "395861e6-a2f3-408b-e592-370b0d74b22b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# print 10 words in vocab dict\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 addition\n",
            "1 anyone\n",
            "2 body\n",
            "3 bricklin\n",
            "4 called\n",
            "5 car\n",
            "6 could\n",
            "7 day\n",
            "8 doors\n",
            "9 early\n",
            "10 engine\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLg2gpyTx7fx",
        "colab_type": "code",
        "outputId": "716032a9-bda9-43f2-8021-ec072c94a489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dictionary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHqhM3Vax9tG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Remove very rare and very common words:\n",
        "\n",
        "- words appearing less than 15 times\n",
        "- words appearing in more than 30% of all documents\n",
        "'''\n",
        "dictionary.filter_extremes(no_below=10, no_above=0.1, keep_n = 8000) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7-ND_FjyFdA",
        "colab_type": "code",
        "outputId": "2077b25d-1c7c-451c-d0e3-c8beb0e5932f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dictionary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H11YL_jdnBT2",
        "colab_type": "text"
      },
      "source": [
        "##### bow vectorize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv1mvLgAz2ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
        "words and how many times those words appear. Save this to 'bow_corpus'\n",
        "'''\n",
        "\n",
        "bow_corpus_train = [dictionary.doc2bow(doc) for doc in data_train]\n",
        "bow_corpus_test =[dictionary.doc2bow(doc) for doc in data_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH7BSnSn0E5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Preview BOW for our sample preprocessed document\n",
        "'''\n",
        "document_num = 10\n",
        "bow_doc_x = bow_corpus_train[document_num]\n",
        "\n",
        "for i in range(len(bow_doc_x)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
        "                                                     dictionary[bow_doc_x[i][0]], \n",
        "                                                     bow_doc_x[i][1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4h7rKvR0kPB",
        "colab_type": "text"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuSmAcYG09ap",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToZIiVuH0m_R",
        "colab_type": "code",
        "outputId": "3ffd604f-cf7a-4431-9d97-b9e445fd8f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# LDA multicore \n",
        "%%time\n",
        "'''\n",
        "Train your lda model using gensim.models.LdaMulticore and save it to 'lda_model'\n",
        "'''\n",
        "lda_model =  gensim.models.LdaMulticore(bow_corpus_train, \n",
        "                                   num_topics = 20, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 10,\n",
        "                                   workers = 2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 3s, sys: 11.5 s, total: 1min 15s\n",
            "Wall time: 1min 45s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU0SlKWArbic",
        "colab_type": "code",
        "outputId": "92e3d920-8927-487b-e210-d06679af50df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "For each topic, we will explore the words occuring in that topic and its relative weight\n",
        "'''\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    # print(topic)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.010*\"government\" + 0.008*\"state\" + 0.007*\"president\" + 0.006*\"israel\" + 0.005*\"country\" + 0.004*\"israeli\" + 0.004*\"american\" + 0.004*\"war\" + 0.004*\"force\" + 0.003*\"turkish\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.019*\"god\" + 0.015*\"jesus\" + 0.007*\"church\" + 0.007*\"believe\" + 0.006*\"word\" + 0.005*\"bible\" + 0.005*\"point\" + 0.005*\"love\" + 0.005*\"sin\" + 0.005*\"mean\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.034*\"game\" + 0.015*\"win\" + 0.012*\"la\" + 0.012*\"score\" + 0.011*\"hit\" + 0.010*\"player\" + 0.010*\"van\" + 0.009*\"cal\" + 0.009*\"team\" + 0.009*\"pitch\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.019*\"image\" + 0.015*\"program\" + 0.014*\"color\" + 0.011*\"display\" + 0.011*\"window\" + 0.007*\"package\" + 0.007*\"thanks\" + 0.007*\"file\" + 0.006*\"graphic\" + 0.006*\"software\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.012*\"book\" + 0.008*\"do\" + 0.008*\"learn\" + 0.007*\"job\" + 0.006*\"new\" + 0.006*\"font\" + 0.006*\"build\" + 0.005*\"moon\" + 0.005*\"lot\" + 0.005*\"language\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.046*\"file\" + 0.045*\"x_x\" + 0.017*\"program\" + 0.016*\"entry\" + 0.010*\"line\" + 0.009*\"name\" + 0.009*\"section\" + 0.008*\"number\" + 0.008*\"read\" + 0.007*\"follow\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.023*\"car\" + 0.011*\"bike\" + 0.007*\"engine\" + 0.007*\"buy\" + 0.005*\"mile\" + 0.005*\"new\" + 0.005*\"cost\" + 0.005*\"back\" + 0.004*\"tire\" + 0.004*\"lot\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.011*\"armenian\" + 0.008*\"start\" + 0.007*\"tell\" + 0.007*\"kill\" + 0.007*\"leave\" + 0.005*\"happen\" + 0.005*\"saw\" + 0.005*\"woman\" + 0.005*\"back\" + 0.005*\"day\"\n",
            "\n",
            "\n",
            "Topic: 8 \n",
            "Words: 0.012*\"thanks\" + 0.008*\"driver\" + 0.008*\"computer\" + 0.007*\"anyone\" + 0.007*\"post\" + 0.007*\"tell\" + 0.006*\"read\" + 0.006*\"cpu\" + 0.006*\"anyone_know\" + 0.006*\"duo\"\n",
            "\n",
            "\n",
            "Topic: 9 \n",
            "Words: 0.033*\"drive\" + 0.015*\"post\" + 0.014*\"group\" + 0.007*\"jumper\" + 0.006*\"discussion\" + 0.006*\"newsgroup\" + 0.006*\"please\" + 0.005*\"read\" + 0.005*\"really\" + 0.005*\"single\"\n",
            "\n",
            "\n",
            "Topic: 10 \n",
            "Words: 0.019*\"team\" + 0.011*\"game\" + 0.011*\"play\" + 0.009*\"player\" + 0.009*\"year\" + 0.006*\"season\" + 0.006*\"hockey\" + 0.006*\"nhl\" + 0.005*\"league\" + 0.005*\"goal\"\n",
            "\n",
            "\n",
            "Topic: 11 \n",
            "Words: 0.014*\"god\" + 0.009*\"christian\" + 0.008*\"believe\" + 0.007*\"religion\" + 0.007*\"mean\" + 0.006*\"argument\" + 0.006*\"must\" + 0.005*\"belief\" + 0.005*\"claim\" + 0.005*\"question\"\n",
            "\n",
            "\n",
            "Topic: 12 \n",
            "Words: 0.008*\"post\" + 0.005*\"life\" + 0.005*\"others\" + 0.004*\"human\" + 0.004*\"show\" + 0.004*\"water\" + 0.004*\"claim\" + 0.004*\"picture\" + 0.004*\"point\" + 0.004*\"face\"\n",
            "\n",
            "\n",
            "Topic: 13 \n",
            "Words: 0.016*\"price\" + 0.015*\"sell\" + 0.014*\"sale\" + 0.012*\"new\" + 0.009*\"buy\" + 0.008*\"cd\" + 0.007*\"ask\" + 0.007*\"include\" + 0.007*\"trade\" + 0.006*\"interested\"\n",
            "\n",
            "\n",
            "Topic: 14 \n",
            "Words: 0.027*\"gun\" + 0.011*\"firearm\" + 0.011*\"law\" + 0.009*\"crime\" + 0.009*\"criminal\" + 0.008*\"weapon\" + 0.007*\"kill\" + 0.006*\"drug\" + 0.006*\"state\" + 0.006*\"police\"\n",
            "\n",
            "\n",
            "Topic: 15 \n",
            "Words: 0.012*\"information\" + 0.012*\"space\" + 0.008*\"data\" + 0.007*\"include\" + 0.007*\"program\" + 0.006*\"nasa\" + 0.005*\"file\" + 0.005*\"list\" + 0.005*\"satellite\" + 0.005*\"send\"\n",
            "\n",
            "\n",
            "Topic: 16 \n",
            "Words: 0.009*\"wire\" + 0.006*\"system\" + 0.006*\"circuit\" + 0.006*\"ground\" + 0.005*\"device\" + 0.005*\"power\" + 0.005*\"data\" + 0.005*\"run\" + 0.004*\"design\" + 0.004*\"new\"\n",
            "\n",
            "\n",
            "Topic: 17 \n",
            "Words: 0.015*\"key\" + 0.010*\"system\" + 0.007*\"program\" + 0.007*\"server\" + 0.006*\"include\" + 0.006*\"available\" + 0.006*\"run\" + 0.006*\"application\" + 0.005*\"information\" + 0.005*\"chip\"\n",
            "\n",
            "\n",
            "Topic: 18 \n",
            "Words: 0.006*\"patient\" + 0.006*\"study\" + 0.006*\"cause\" + 0.005*\"food\" + 0.005*\"doctor\" + 0.005*\"pain\" + 0.004*\"since\" + 0.004*\"disease\" + 0.004*\"day\" + 0.004*\"case\"\n",
            "\n",
            "\n",
            "Topic: 19 \n",
            "Words: 0.012*\"card\" + 0.011*\"system\" + 0.010*\"do\" + 0.009*\"window\" + 0.009*\"run\" + 0.007*\"disk\" + 0.007*\"driver\" + 0.006*\"mac\" + 0.006*\"pc\" + 0.006*\"drive\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfFG3Xsm0u12",
        "colab_type": "code",
        "outputId": "4ac35bb4-f18f-481d-fdcb-ee21abb5a4c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''\n",
        "For each topic, we will explore the words occuring in that topic and its relative weight\n",
        "'''\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    # print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(topic)\n",
        "    print(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.048*\"car\" + 0.010*\"tire\" + 0.009*\"model\" + 0.007*\"dealer\" + 0.007*\"anyone\" + 0.006*\"thanks\" + 0.006*\"owner\" + 0.005*\"bmw\" + 0.005*\"brake\" + 0.005*\"trade\"\n",
            "\n",
            "\n",
            "0.013*\"application\" + 0.013*\"widget\" + 0.012*\"window\" + 0.010*\"motif\" + 0.010*\"set\" + 0.009*\"server\" + 0.008*\"display\" + 0.008*\"run\" + 0.008*\"code\" + 0.008*\"program\"\n",
            "\n",
            "\n",
            "0.007*\"water\" + 0.006*\"etc\" + 0.005*\"book\" + 0.005*\"university\" + 0.004*\"post\" + 0.004*\"anyone\" + 0.004*\"question\" + 0.004*\"new\" + 0.004*\"heat\" + 0.004*\"metal\"\n",
            "\n",
            "\n",
            "0.020*\"la\" + 0.018*\"van\" + 0.016*\"bos\" + 0.016*\"det\" + 0.015*\"cal\" + 0.014*\"pit\" + 0.014*\"tor\" + 0.013*\"min\" + 0.013*\"nj\" + 0.012*\"mon\"\n",
            "\n",
            "\n",
            "0.015*\"sale\" + 0.013*\"sell\" + 0.009*\"price\" + 0.007*\"cd\" + 0.007*\"new\" + 0.007*\"ask\" + 0.007*\"interested\" + 0.007*\"include\" + 0.006*\"line\" + 0.006*\"printer\"\n",
            "\n",
            "\n",
            "0.040*\"post\" + 0.014*\"article\" + 0.014*\"drug\" + 0.009*\"read\" + 0.009*\"group\" + 0.007*\"anyone\" + 0.007*\"newsgroup\" + 0.006*\"dod\" + 0.006*\"edge\" + 0.005*\"please\"\n",
            "\n",
            "\n",
            "0.008*\"really\" + 0.007*\"point\" + 0.006*\"better\" + 0.005*\"mean\" + 0.005*\"sure\" + 0.005*\"maybe\" + 0.005*\"keep\" + 0.004*\"start\" + 0.004*\"might\" + 0.004*\"still\"\n",
            "\n",
            "\n",
            "0.014*\"armenian\" + 0.007*\"kill\" + 0.006*\"tell\" + 0.005*\"turkish\" + 0.005*\"live\" + 0.004*\"woman\" + 0.004*\"happen\" + 0.004*\"fire\" + 0.004*\"leave\" + 0.004*\"turkey\"\n",
            "\n",
            "\n",
            "0.020*\"game\" + 0.018*\"team\" + 0.011*\"player\" + 0.010*\"play\" + 0.008*\"year\" + 0.006*\"season\" + 0.006*\"win\" + 0.005*\"league\" + 0.005*\"score\" + 0.005*\"hockey\"\n",
            "\n",
            "\n",
            "0.024*\"file\" + 0.018*\"x_x\" + 0.017*\"program\" + 0.013*\"image\" + 0.008*\"include\" + 0.008*\"information\" + 0.008*\"format\" + 0.006*\"entry\" + 0.006*\"available\" + 0.006*\"data\"\n",
            "\n",
            "\n",
            "0.015*\"god\" + 0.007*\"believe\" + 0.007*\"christian\" + 0.005*\"mean\" + 0.005*\"religion\" + 0.005*\"true\" + 0.005*\"word\" + 0.004*\"life\" + 0.004*\"claim\" + 0.004*\"bible\"\n",
            "\n",
            "\n",
            "0.013*\"space\" + 0.007*\"satellite\" + 0.007*\"launch\" + 0.006*\"mission\" + 0.006*\"system\" + 0.006*\"orbit\" + 0.006*\"nasa\" + 0.005*\"key\" + 0.005*\"data\" + 0.005*\"earth\"\n",
            "\n",
            "\n",
            "0.013*\"state\" + 0.013*\"president\" + 0.010*\"law\" + 0.006*\"consider\" + 0.006*\"group\" + 0.006*\"tax\" + 0.005*\"government\" + 0.005*\"gun\" + 0.005*\"bill\" + 0.005*\"mean\"\n",
            "\n",
            "\n",
            "0.021*\"drive\" + 0.009*\"system\" + 0.009*\"wire\" + 0.006*\"power\" + 0.006*\"disk\" + 0.005*\"chip\" + 0.005*\"scsi\" + 0.005*\"run\" + 0.005*\"circuit\" + 0.004*\"speed\"\n",
            "\n",
            "\n",
            "0.014*\"key\" + 0.012*\"government\" + 0.008*\"system\" + 0.007*\"chip\" + 0.007*\"new\" + 0.006*\"encryption\" + 0.006*\"program\" + 0.005*\"law_enforcement\" + 0.004*\"clipper_chip\" + 0.004*\"nsa\"\n",
            "\n",
            "\n",
            "0.013*\"bike\" + 0.011*\"car\" + 0.007*\"back\" + 0.006*\"engine\" + 0.005*\"mile\" + 0.005*\"leave\" + 0.005*\"buy\" + 0.005*\"turn\" + 0.005*\"put\" + 0.004*\"ride\"\n",
            "\n",
            "\n",
            "0.015*\"window\" + 0.012*\"run\" + 0.012*\"do\" + 0.010*\"system\" + 0.009*\"file\" + 0.009*\"driver\" + 0.008*\"card\" + 0.008*\"program\" + 0.006*\"machine\" + 0.005*\"windows\"\n",
            "\n",
            "\n",
            "0.008*\"study\" + 0.006*\"patient\" + 0.006*\"cause\" + 0.005*\"report\" + 0.005*\"case\" + 0.005*\"gun\" + 0.005*\"disease\" + 0.005*\"food\" + 0.004*\"firearm\" + 0.004*\"person\"\n",
            "\n",
            "\n",
            "0.023*\"jesus\" + 0.011*\"god\" + 0.011*\"law\" + 0.010*\"day\" + 0.008*\"israel\" + 0.007*\"die\" + 0.006*\"hell\" + 0.005*\"matthew\" + 0.005*\"israeli\" + 0.005*\"paul\"\n",
            "\n",
            "\n",
            "0.011*\"software\" + 0.010*\"system\" + 0.008*\"mac\" + 0.007*\"book\" + 0.007*\"copy\" + 0.007*\"fax\" + 0.006*\"computer\" + 0.006*\"apple\" + 0.006*\"information\" + 0.006*\"price\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "majxQ5CR9Gvl",
        "colab_type": "text"
      },
      "source": [
        "#### Topic Coherence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBagJ2SBFZiV",
        "colab_type": "code",
        "outputId": "c66c2d5f-3f96-4096-d665-b668136fbb2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from gensim.models import CoherenceModel\n",
        "coherence_model_lda = CoherenceModel(model=lda_model,\n",
        "                                    texts = data_train, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score:', coherence_lda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score: 0.4743163706982658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hzFIx0sFxaU",
        "colab_type": "code",
        "outputId": "a1efd5b7-ccd5-4b73-f29b-1be44e6a1bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "coherence_model_lda_test = CoherenceModel(model=lda_model,\n",
        "                                    texts = data_test, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "coherence_lda_test = coherence_model_lda_test.get_coherence()\n",
        "print('\\nCoherence Score on test:', coherence_lda_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score on test: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/topic_coherence/direct_confirmation_measure.py:195: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  m_lr_i = np.log(numerator / denominator)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/topic_coherence/indirect_confirmation_measure.py:317: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ZFwVOULmXQ",
        "colab_type": "code",
        "outputId": "36a9bfca-7e01-4c3b-98ff-f9762dffe82c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.isnan(coherence_lda_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TgoCt9KLAeD",
        "colab_type": "code",
        "outputId": "bd8ad802-0f9a-4fd9-e6e7-fbc040553547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from gensim.models import CoherenceModel\n",
        "coherence_lda_test = np.nan\n",
        "while np.isnan(coherence_lda_test):\n",
        "  lda_model =  gensim.models.LdaMulticore(bow_corpus_train, \n",
        "                                    num_topics = 20, \n",
        "                                    id2word = dictionary,                                    \n",
        "                                    passes = 10,\n",
        "                                    workers = 2)\n",
        "  coherence_model_lda = CoherenceModel(model=lda_model,\n",
        "                                    texts = data_train, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "  coherence_lda = coherence_model_lda.get_coherence()\n",
        "  print('\\nCoherence Score:', coherence_lda)\n",
        "\n",
        "  coherence_model_lda_test = CoherenceModel(model=lda_model,\n",
        "                                      texts = data_test, \n",
        "                                      dictionary = dictionary,\n",
        "                                      coherence='c_v')\n",
        "  coherence_lda_test = coherence_model_lda_test.get_coherence()\n",
        "  print('\\nCoherence Score on test:', coherence_lda_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score: 0.5436908306531877\n",
            "\n",
            "Coherence Score on test: 0.4879053489457356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0flVrzcRFaHz",
        "colab_type": "text"
      },
      "source": [
        "===========================================  \n",
        "yesterday"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6YAp_eNJSRr",
        "colab_type": "code",
        "outputId": "6d944b58-14d8-4558-c187-35e8043acd05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#vocab 6,000\n",
        "from gensim.models import CoherenceModel\n",
        "coherence_model_lda = CoherenceModel(model=lda_model,\n",
        "                                    texts = data_train, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score:', coherence_lda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score: 0.5670714144282154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMT_afKoLL_N",
        "colab_type": "code",
        "outputId": "65ff6b2f-7a34-4827-ea20-297d2c6cd90f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "coherence_model_lda_test = CoherenceModel(model=lda_model,\n",
        "                                    texts = data_test, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "coherence_lda_test = coherence_model_lda_test.get_coherence()\n",
        "print('\\nCoherence Score on test:', coherence_lda_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score on test: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91fhfcuvR2iY",
        "colab_type": "code",
        "outputId": "be3f5388-a587-462d-cdb7-d00414785b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#vocab 10,000\n",
        "from gensim.models import CoherenceModel\n",
        "coherence_model_lda = CoherenceModel(model=lda_model,\n",
        "                                    texts = data_train, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score:', coherence_lda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score: 0.4869315584837187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY_X6QtYR49g",
        "colab_type": "code",
        "outputId": "430dc357-ea83-4b90-9e5a-2342e75623e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "\n",
        "coherence_model_lda_test = CoherenceModel(model=lda_model,\n",
        "                                    texts = data_test, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "coherence_lda_test = coherence_model_lda_test.get_coherence()\n",
        "print('\\nCoherence Score on test:', coherence_lda_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/topic_coherence/direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  m_lr_i = np.log(numerator / denominator)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/topic_coherence/indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score on test: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RoDobQBMj--",
        "colab_type": "text"
      },
      "source": [
        "===================================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Rt1rOr1C8p",
        "colab_type": "code",
        "outputId": "81ef87f9-877b-46b6-febd-afb34de83653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#vocab 10,000\n",
        "from gensim.models import CoherenceModel\n",
        "coherence_model_lda = CoherenceModel(model=lda_model,\n",
        "                                    texts = data_train, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score:', coherence_lda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score: 0.47576536775224626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_scgqsEJ1NJN",
        "colab_type": "code",
        "outputId": "e33856c7-8f2b-481b-c187-5b6f8a937a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from gensim.models import CoherenceModel\n",
        "coherence_model_lda_test = CoherenceModel(model=lda_model,\n",
        "                                    texts = data_test, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "coherence_lda_test = coherence_model_lda_test.get_coherence()\n",
        "print('\\nCoherence Score on test:', coherence_lda_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score on test: 0.44858534521467136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O1_FGVS3ddI",
        "colab_type": "text"
      },
      "source": [
        "======================================================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-71V1aunFKzK",
        "colab_type": "text"
      },
      "source": [
        "#### Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuM6DdjXgKJP",
        "colab_type": "code",
        "outputId": "beda725c-e4a3-45c5-d2ee-02c0e335bd3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "perplexity = lda_model.log_perplexity(bow_corpus_train)\n",
        "print('lda perplexity:', perplexity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lda perplexity: -8.016822289837787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLLt2-klZwry",
        "colab_type": "text"
      },
      "source": [
        "## HDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6BXFeunQcFm",
        "colab_type": "code",
        "outputId": "802437a2-d2ea-4add-b608-30bedf143bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "hdp_model.evaluate_test_corpus(bow_corpus_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-3845594.7109833304"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHYO67DwQN-l",
        "colab_type": "text"
      },
      "source": [
        "========================================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Ei8oKP4ti8",
        "colab_type": "code",
        "outputId": "f88a529d-d3dd-48f5-c55e-0e398585e106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "hdp_model = gensim.models.HdpModel(bow_corpus_train, dictionary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 53.5 s, sys: 34.4 s, total: 1min 27s\n",
            "Wall time: 46.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE6zDlKpYI6X",
        "colab_type": "code",
        "outputId": "348bd7b6-b3e9-4ce8-c8e7-97c42525dc64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "hdp_model.print_topics()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.004*system + 0.004*file + 0.004*program + 0.003*new + 0.003*include + 0.003*run + 0.002*information + 0.002*support + 0.002*number + 0.002*year'),\n",
              " (1,\n",
              "  '0.005*god + 0.003*believe + 0.003*point + 0.003*mean + 0.003*tell + 0.002*jesus + 0.002*car + 0.002*really + 0.002*still + 0.002*start'),\n",
              " (2,\n",
              "  '0.005*include + 0.004*file + 0.004*system + 0.004*program + 0.003*available + 0.003*run + 0.003*version + 0.003*machine + 0.003*support + 0.003*software'),\n",
              " (3,\n",
              "  '0.031*x_x + 0.006*program + 0.005*file + 0.004*line + 0.003*write + 0.003*read + 0.003*return + 0.003*open + 0.002*year + 0.002*include'),\n",
              " (4,\n",
              "  '0.002*file + 0.002*vote + 0.002*image + 0.002*include + 0.002*name + 0.001*system + 0.001*mean + 0.001*post + 0.001*word + 0.001*new'),\n",
              " (5,\n",
              "  '0.004*det + 0.004*pit + 0.004*van + 0.004*tor + 0.004*nj + 0.004*nyr + 0.004*la + 0.004*chi + 0.004*que + 0.004*nyi'),\n",
              " (6,\n",
              "  '0.002*car + 0.002*drive + 0.002*year + 0.001*card + 0.001*insurance + 0.001*chipsets + 0.001*run + 0.001*rate + 0.001*buy + 0.001*pay'),\n",
              " (7,\n",
              "  '0.002*jesus + 0.001*run + 0.001*car + 0.001*act + 0.001*either + 0.001*system + 0.001*better + 0.001*ask + 0.001*drive + 0.001*talk'),\n",
              " (8,\n",
              "  '0.001*father + 0.001*son + 0.001*holy_spirit + 0.001*creed + 0.001*jim + 0.001*since + 0.001*god + 0.001*back + 0.001*post + 0.001*point'),\n",
              " (9,\n",
              "  '0.005*power_play + 0.004*pt + 0.004*pp + 0.003*first_period + 0.002*second_period + 0.002*pittsburgh + 0.002*vancouver + 0.002*calgary + 0.002*third_period + 0.002*score'),\n",
              " (10,\n",
              "  '0.001*fan + 0.001*day + 0.001*team + 0.001*etc + 0.001*grayscale + 0.001*christian + 0.001*man + 0.001*believe + 0.001*include + 0.001*new'),\n",
              " (11,\n",
              "  '0.001*game + 0.001*team + 0.001*point + 0.001*weaver + 0.001*season + 0.001*registration + 0.001*case + 0.001*hartford + 0.001*since + 0.001*year'),\n",
              " (12,\n",
              "  '0.002*nasa + 0.001*degree + 0.001*program + 0.001*astronaut + 0.001*include + 0.001*application + 0.001*flight + 0.001*source + 0.001*reverse_engineer + 0.001*pilot'),\n",
              " (13,\n",
              "  '0.001*meaningless + 0.001*sufficiently + 0.001*ask_whether + 0.001*cure + 0.001*team + 0.001*point + 0.001*noisy + 0.001*never + 0.001*raise + 0.001*et'),\n",
              " (14,\n",
              "  '0.001*baud_modem + 0.001*partial + 0.001*ssf + 0.001*put + 0.001*hit + 0.001*side + 0.001*give_hint + 0.001*quite + 0.001*stall + 0.001*head'),\n",
              " (15,\n",
              "  '0.001*shuttle + 0.001*roll + 0.001*mediocre + 0.001*attitude + 0.001*difference + 0.001*branch_davidians + 0.001*component + 0.001*software + 0.001*roy + 0.001*maneuver'),\n",
              " (16,\n",
              "  '0.001*high_speed + 0.001*soil + 0.001*defective + 0.001*steve + 0.001*slave + 0.001*scsi_controller + 0.001*bitch + 0.001*plasma + 0.001*kurri + 0.001*team'),\n",
              " (17,\n",
              "  '0.001*fine + 0.001*do + 0.001*trace + 0.001*coalition + 0.001*september + 0.001*test + 0.001*preference + 0.001*asshole + 0.001*operating + 0.001*invalid'),\n",
              " (18,\n",
              "  '0.001*baku + 0.001*armenian + 0.001*leave + 0.001*driver + 0.001*offensively + 0.001*city + 0.001*back + 0.001*sumgait + 0.001*sense + 0.001*car'),\n",
              " (19,\n",
              "  '0.001*system + 0.001*tremendous + 0.001*warp + 0.001*distinction + 0.001*probably + 0.001*society + 0.001*finally_get + 0.001*two_month + 0.001*effectively + 0.001*mechanical')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkZkS1io5AYD",
        "colab_type": "text"
      },
      "source": [
        "=================================================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbKNor8PghK0",
        "colab_type": "code",
        "outputId": "973db0ca-c8eb-4c2b-9579-c071190a408d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "coherence_model_hdp = CoherenceModel(model=hdp_model,\n",
        "                                    texts = data_train, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "coherence_hdp = coherence_model_hdp.get_coherence()\n",
        "print('\\nCoherence Score:', coherence_hdp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score: 0.6399517430396299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p1a9E5XUzUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "122e0d8a-f974-4bb0-dd1c-70da29a26708"
      },
      "source": [
        "coherence_model_hdp_test = CoherenceModel(model=hdp_model,\n",
        "                                    texts = data_test, \n",
        "                                    dictionary = dictionary,\n",
        "                                    coherence='c_v')\n",
        "coherence_hdp_test = coherence_model_hdp_test.get_coherence()\n",
        "print('\\nCoherence Score:', coherence_hdp_test)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score: 0.506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kcotZr2iuxA",
        "colab_type": "text"
      },
      "source": [
        "## CTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQKoOto8iwxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBZAXSv9isYY",
        "colab_type": "text"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKu_mYNJakSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpfMo5dJtSud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade gensim\n",
        "special_tokens = {'pad': 0, 'unk':1}\n",
        "dictionary.patch_with_special_tokens(special_tokens)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYS-09y-Aw6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UNK_IDX = 1\n",
        "token2id = dictionary.token2id\n",
        "vocab_size = len(dictionary)\n",
        "def token2index_dataset(tokens_data, token2id):\n",
        "    indices_data = []\n",
        "    for tokens in tokens_data:\n",
        "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
        "        indices_data.append(index_list)\n",
        "    return indices_data\n",
        "\n",
        "x_train = token2index_dataset(data_train, token2id)\n",
        "x_train = np.array([np.array(document) for document in x_train])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe2JG30YBxuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def onehot(data, min_length):\n",
        "    return np.bincount(data, minlength=min_length)\n",
        "\n",
        "x_train = np.array([onehot(doc.astype('int'),vocab_size) for doc in x_train if np.sum(doc)!=0])\n",
        "n_samples_tr = x_train.shape[0]\n",
        "docs_tr = x_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dxp70cjjN4K4",
        "colab": {}
      },
      "source": [
        "def make_network(layer1=100,layer2=100,num_topics=20,batch_size=200,learning_rate=0.01):\n",
        "    tf.reset_default_graph()\n",
        "    network_architecture = \\\n",
        "        dict(n_hidden_recog_1=layer1, # 1st layer encoder neurons\n",
        "             n_hidden_recog_2=layer2, # 2nd layer encoder neurons\n",
        "             n_hidden_gener_1=x_train.shape[1], # 1st layer decoder neurons\n",
        "             n_input=x_train.shape[1], # MNIST data input (img shape: 28*28)\n",
        "             n_z=num_topics)  # dimensionality of latent space\n",
        "\n",
        "    return network_architecture,batch_size,learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LqSxWvBB6uU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_val = token2index_dataset(data_test, token2id)\n",
        "x_val = np.array([np.array(document) for document in x_val])\n",
        "x_val = np.array([onehot(doc.astype('int'),vocab_size) for doc in x_val if np.sum(doc)!=0])\n",
        "n_samples_te = x_val.shape[0]\n",
        "docs_te = x_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJiQ7Aa1CHuK",
        "colab_type": "code",
        "outputId": "17c5ecf5-6232-4961-d1f8-786680ddd38a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "slim = tf.contrib.slim\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class VAE(object):\n",
        "    \"\"\"\n",
        "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus,\n",
        "                 learning_rate=0.01, batch_size=100):\n",
        "        self.network_architecture = network_architecture\n",
        "        self.transfer_fct = transfer_fct\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        print('Initial Learning Rate:', self.learning_rate)\n",
        "\n",
        "        # tf Graph input\n",
        "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]], name='input')\n",
        "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "        self.h_dim = (network_architecture[\"n_z\"]) # had a float before\n",
        "        self.a = 1*np.ones((1 , self.h_dim)).astype(np.float32)                         # a    = 1\n",
        "        self.prior_mean = tf.constant((np.log(self.a).T-np.mean(np.log(self.a),1)).T)          # prior_mean  = 0\n",
        "        self.prior_var = tf.constant(  ( ( (1.0/self.a)*( 1 - (2.0/self.h_dim) ) ).T +       # prior_var = 0.99 + 0.005 = 0.995\n",
        "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a,1) ).T  )\n",
        "        self.prior_logvar = tf.log(self.prior_var)\n",
        "        self.means = []\n",
        "\n",
        "        self._create_network()\n",
        "        with tf.name_scope('cost'):\n",
        "            self._create_loss_optimizer()\n",
        "\n",
        "        init = tf.initialize_all_variables()\n",
        "\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    def _create_network(self):\n",
        "        \"\"\"\n",
        "        steps:\n",
        "        1. initialize weights\n",
        "        2. build recognition network\n",
        "        3. build reconstruction network\n",
        "        \"\"\"\n",
        "        n_z = self.network_architecture['n_z']\n",
        "        n_hidden_gener_1 = self.network_architecture['n_hidden_gener_1']\n",
        "        en1 = slim.layers.linear(self.x, self.network_architecture['n_hidden_recog_1'], scope='FC_en1')\n",
        "        en1 = tf.nn.softplus(en1, name='softplus1')\n",
        "        en2 = slim.layers.linear(en1,    self.network_architecture['n_hidden_recog_2'], scope='FC_en2')\n",
        "        en2 = tf.nn.softplus(en2, name='softplus2')\n",
        "        en2_do = slim.layers.dropout(en2, self.keep_prob, scope='en2_dropped')\n",
        "        self.posterior_mean   = slim.layers.linear(en2_do, self.network_architecture['n_z'], scope='FC_mean')\n",
        "        self.posterior_logvar = slim.layers.linear(en2_do, self.network_architecture['n_z'], scope='FC_logvar')\n",
        "        self.posterior_mean   = slim.layers.batch_norm(self.posterior_mean, scope='BN_mean')\n",
        "        self.posterior_logvar = slim.layers.batch_norm(self.posterior_logvar, scope='BN_logvar')\n",
        "        \n",
        "        with tf.name_scope('z_scope'):\n",
        "            eps = tf.random_normal((self.batch_size, n_z), 0, 1,                            # take noise\n",
        "                                   dtype=tf.float32)\n",
        "            self.z = tf.add(self.posterior_mean,\n",
        "                            tf.multiply(tf.sqrt(tf.exp(self.posterior_logvar)), eps))         # reparameterization z\n",
        "            self.posterior_var = tf.exp(self.posterior_logvar) \n",
        "\n",
        "        self.p = slim.layers.softmax(self.z)\n",
        "        p_do = slim.layers.dropout(self.p, self.keep_prob, scope='p_dropped')               # dropout(softmax(z))\n",
        "        decoded = slim.layers.linear(p_do, n_hidden_gener_1, scope='FC_decoder')\n",
        "\n",
        "        self.x_reconstr_mean = tf.nn.softmax(slim.layers.batch_norm(decoded, scope='BN_decoder'))                    # softmax(bn(50->1995))\n",
        "\n",
        "        print(self.x_reconstr_mean)\n",
        "\n",
        "    def _create_loss_optimizer(self):\n",
        "\n",
        "        tensor = self.x * tf.log(self.x_reconstr_mean+1e-10)                                                   # prevent log(0)\n",
        "        indices = [i for i in range(1,tensor.shape[1])] # exclude 'pad', include 'unk'\n",
        "        result = tf.gather(tensor, indices, axis=1)\n",
        "        NL = -tf.reduce_sum(result, 1)\n",
        "        # NL = -tf.reduce_sum(self.x * tf.log(self.x_reconstr_mean+1e-10), 1)     # cross entropy on categorical:- sum(ylog(p))\n",
        "\n",
        "        var_division    = self.posterior_var  / self.prior_var\n",
        "        diff            = self.posterior_mean - self.prior_mean\n",
        "        diff_term       = diff * diff / self.prior_var\n",
        "        logvar_division = self.prior_logvar - self.posterior_logvar\n",
        "        KLD = 0.5 * (tf.reduce_sum(var_division + diff_term + logvar_division, 1) - self.h_dim )\n",
        "\n",
        "        self.cost = tf.reduce_mean(NL + KLD)\n",
        " \n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.99,epsilon=0.01).minimize(self.cost)\n",
        "\n",
        "    def partial_fit(self, X):\n",
        "\n",
        "        #if hasattr(self, 'decoder_weight'):\n",
        "            #decoder_weight = self.decoder_weight\n",
        "        #else:\n",
        "        decoder_weight = [v for v in tf.global_variables() if v.name=='FC_decoder/weights:0'][0]\n",
        "        opt, cost,emb,p = self.sess.run((self.optimizer, self.cost, decoder_weight, self.p),feed_dict={self.x: X,self.keep_prob: .8})\n",
        "        # print(self.sess.run((self.p),feed_dict={self.x: X,self.keep_prob: .8}))\n",
        "        return cost,emb,p\n",
        "\n",
        "    def test(self, X):\n",
        "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
        "        \"\"\"\n",
        "        cost = self.sess.run((self.cost),feed_dict={self.x: np.expand_dims(X, axis=0),self.keep_prob: 1.0})\n",
        "        return cost\n",
        "    def topic_prop(self, X):\n",
        "        \"\"\"heta_ is the topic proportion vector. Apply softmax transformation to it before use.\n",
        "        \"\"\"\n",
        "        theta_ = self.sess.run((self.z),feed_dict={self.x: np.expand_dims(X, axis=0),self.keep_prob: 1.0})\n",
        "        return theta_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YaUALWnBN4K5",
        "colab": {}
      },
      "source": [
        "def make_network(layer1=100,layer2=100,num_topics=20,batch_size=200,learning_rate=0.01):\n",
        "    tf.reset_default_graph()\n",
        "    network_architecture = \\\n",
        "        dict(n_hidden_recog_1=layer1, # 1st layer encoder neurons\n",
        "             n_hidden_recog_2=layer2, # 2nd layer encoder neurons\n",
        "             n_hidden_gener_1=x_train.shape[1], # 1st layer decoder neurons\n",
        "             n_input=x_train.shape[1], # MNIST data input (img shape: 28*28)\n",
        "             n_z=num_topics)  # dimensionality of latent space\n",
        "\n",
        "    return network_architecture,batch_size,learning_rate\n",
        "    \n",
        "def create_minibatch(data,batch_size):\n",
        "    rng = np.random.RandomState(10)\n",
        "\n",
        "    while True:\n",
        "        # Return random data samples of a size 'minibatch_size' at each iteration\n",
        "        ixs = rng.randint(data.shape[0], size=batch_size)\n",
        "        yield data[ixs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RCA-d2_gN4K7",
        "colab": {}
      },
      "source": [
        "def train(network_architecture, minibatches, type='prodlda',learning_rate=0.01,\n",
        "          batch_size=200, training_epochs=100, display_step=5):\n",
        "    tf.reset_default_graph()\n",
        "    vae = VAE(network_architecture,transfer_fct=tf.nn.softplus,\n",
        "                learning_rate=learning_rate, batch_size=batch_size)\n",
        "    writer = tf.summary.FileWriter('logs', tf.get_default_graph())\n",
        "    emb=0\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(n_samples_tr / batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_xs = next(minibatches)\n",
        "            # Fit training using batch data\n",
        "            cost,emb,p = vae.partial_fit(batch_xs)\n",
        "            # Compute average loss\n",
        "            avg_cost += cost / n_samples_tr * batch_size\n",
        "\n",
        "            if np.isnan(avg_cost):\n",
        "                print(epoch,i,np.sum(batch_xs,1).astype(np.int),batch_xs.shape)\n",
        "                print('Encountered NaN, stopping training. Please check the learning_rate settings and the momentum.')\n",
        "                # return vae,emb\n",
        "                sys.exit()\n",
        "\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \\\n",
        "                  \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "            print_ppl(vae)\n",
        "    return vae,emb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsxKHfmyN4K9",
        "colab": {}
      },
      "source": [
        "def print_top_words(beta, feature_names, n_top_words=10):\n",
        "    print('---------------Printing the Topics------------------')\n",
        "    for i in range(len(beta)):\n",
        "        print(\" \".join([feature_names[j]\n",
        "            for j in beta[i].argsort()[:-n_top_words - 1:-1]]))\n",
        "    print('---------------End of Topics------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E_jExRbwN4LK",
        "colab": {}
      },
      "source": [
        "def print_ppl(model):\n",
        "    cost=[]\n",
        "    for doc in docs_te:\n",
        "        doc = doc.astype('float32')\n",
        "        n_d = np.sum(doc[1:]) # count non-pad\n",
        "        c=model.test(doc)\n",
        "        if n_d==0:\n",
        "            continue\n",
        "        else:\n",
        "            cost.append(c/n_d)\n",
        "    print('The approximated perplexity for test set is: ',(np.exp(np.mean(np.array(cost)))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CfYWBLBpN4LB",
        "outputId": "dffcccd4-e5f7-429a-9b77-b110edf68074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "network_architecture,batch_size,learning_rate=make_network(layer1=200,layer2=100,num_topics=20,\n",
        "                                                           batch_size=256,learning_rate=0.01)\n",
        "network_architecture"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_hidden_gener_1': 8001,\n",
              " 'n_hidden_recog_1': 200,\n",
              " 'n_hidden_recog_2': 100,\n",
              " 'n_input': 8001,\n",
              " 'n_z': 20}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rVJtm0neN4LD",
        "outputId": "39a29eb0-42dc-45ee-8939-e1203cae6ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "%%time\n",
        "minibatches = create_minibatch(docs_tr.astype('float32'),batch_size=batch_size)\n",
        "vae,emb = train(network_architecture, minibatches, type='prodlda',learning_rate=0.01,\n",
        "          batch_size=batch_size, training_epochs=100, display_step=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Learning Rate: 0.01\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "Tensor(\"Softmax_1:0\", shape=(256, 8001), dtype=float32)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Epoch: 0001 cost= 895.774136745\n",
            "The approximated perplexity for test set is:  7046.7983\n",
            "Epoch: 0006 cost= 672.931849190\n",
            "The approximated perplexity for test set is:  3400.3796\n",
            "Epoch: 0011 cost= 652.995368923\n",
            "The approximated perplexity for test set is:  1892.3381\n",
            "Epoch: 0016 cost= 624.165778635\n",
            "The approximated perplexity for test set is:  1434.7699\n",
            "Epoch: 0021 cost= 644.345398627\n",
            "The approximated perplexity for test set is:  1184.8885\n",
            "Epoch: 0026 cost= 604.180534553\n",
            "The approximated perplexity for test set is:  1049.2427\n",
            "Epoch: 0031 cost= 580.621092329\n",
            "The approximated perplexity for test set is:  993.40393\n",
            "Epoch: 0036 cost= 590.637583568\n",
            "The approximated perplexity for test set is:  969.67804\n",
            "Epoch: 0041 cost= 585.489200075\n",
            "The approximated perplexity for test set is:  961.58295\n",
            "Epoch: 0046 cost= 589.266761273\n",
            "The approximated perplexity for test set is:  962.5761\n",
            "Epoch: 0051 cost= 586.615482024\n",
            "The approximated perplexity for test set is:  962.3081\n",
            "Epoch: 0056 cost= 574.546943219\n",
            "The approximated perplexity for test set is:  966.96216\n",
            "Epoch: 0061 cost= 593.460146586\n",
            "The approximated perplexity for test set is:  970.24\n",
            "Epoch: 0066 cost= 566.747430417\n",
            "The approximated perplexity for test set is:  968.92975\n",
            "Epoch: 0071 cost= 575.938642669\n",
            "The approximated perplexity for test set is:  972.4424\n",
            "Epoch: 0076 cost= 591.484824819\n",
            "The approximated perplexity for test set is:  985.97217\n",
            "Epoch: 0081 cost= 561.471902998\n",
            "The approximated perplexity for test set is:  1004.7559\n",
            "Epoch: 0086 cost= 567.253933253\n",
            "The approximated perplexity for test set is:  1020.46655\n",
            "Epoch: 0091 cost= 556.611828043\n",
            "The approximated perplexity for test set is:  1033.0653\n",
            "Epoch: 0096 cost= 565.007125335\n",
            "The approximated perplexity for test set is:  1045.5573\n",
            "CPU times: user 6min 3s, sys: 1min 13s, total: 7min 17s\n",
            "Wall time: 5min 48s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lcOz--9dfYTm",
        "colab": {}
      },
      "source": [
        "def top_words(beta, feature_names, n_top_words=10):\n",
        "    res = []\n",
        "    for i in range(len(beta)):\n",
        "        res.append([feature_names[j] for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "acxSgrm7fm7R",
        "colab": {}
      },
      "source": [
        "topic_words = top_words(emb, list(zip(*sorted(token2id.items(), key=lambda x: x[1])))[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3xJAQghA6gc",
        "colab_type": "code",
        "outputId": "dbefe8ed-ad7c-4f8f-b796-bffe2f1406c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "topic_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['dos',\n",
              "  'windows',\n",
              "  'card',\n",
              "  'scsi',\n",
              "  'apple',\n",
              "  'printer',\n",
              "  'duo',\n",
              "  'cpu',\n",
              "  'keyboard',\n",
              "  'cmos'],\n",
              " ['thanks',\n",
              "  'anyone_know',\n",
              "  'thanks_advance',\n",
              "  'hi',\n",
              "  'windows',\n",
              "  'helmet',\n",
              "  'gordon_banks',\n",
              "  'sale',\n",
              "  'surrender_soon',\n",
              "  'skepticism_chastity'],\n",
              " ['jumper',\n",
              "  'adaptec',\n",
              "  'floppy_disk',\n",
              "  'dos',\n",
              "  'hard_disk',\n",
              "  'drives',\n",
              "  'bios',\n",
              "  'tape_drive',\n",
              "  'jumpers',\n",
              "  'nec'],\n",
              " ['atheists',\n",
              "  'atheism',\n",
              "  'argument',\n",
              "  'militia',\n",
              "  'atheist',\n",
              "  'god',\n",
              "  'conclusion',\n",
              "  'religious',\n",
              "  'must',\n",
              "  'proposition'],\n",
              " ['flyers', 'nhl', 'unk', 'ahl', 'sabres', 'puck', 'que', 'pit', 'nyi', 'sj'],\n",
              " ['chip',\n",
              "  'key_escrow',\n",
              "  'clipper_chip',\n",
              "  'cpsr',\n",
              "  'serial_number',\n",
              "  'nist',\n",
              "  'private_sector',\n",
              "  'astronaut',\n",
              "  'encryption',\n",
              "  'government'],\n",
              " ['wiring',\n",
              "  'outlets',\n",
              "  'wire',\n",
              "  'infections',\n",
              "  'illness',\n",
              "  'public_health',\n",
              "  'q',\n",
              "  'health',\n",
              "  'circuits',\n",
              "  'aids'],\n",
              " ['gordon_banks',\n",
              "  'skepticism_chastity',\n",
              "  'surrender_soon',\n",
              "  'intellect_shameful',\n",
              "  'anyone_know',\n",
              "  'thanks_advance',\n",
              "  'cheers_kent',\n",
              "  'thanks',\n",
              "  'hi',\n",
              "  'helmet'],\n",
              " ['propulsion',\n",
              "  'pgp',\n",
              "  'routines',\n",
              "  'anonymous',\n",
              "  'public_key',\n",
              "  'telescope',\n",
              "  'image_processing',\n",
              "  'package',\n",
              "  'precision',\n",
              "  'data'],\n",
              " ['chip',\n",
              "  'serial_number',\n",
              "  'fpu',\n",
              "  'motherboard',\n",
              "  'screen',\n",
              "  'card',\n",
              "  'phones',\n",
              "  'session_key',\n",
              "  'monitor',\n",
              "  'clipper'],\n",
              " ['new',\n",
              "  'work',\n",
              "  'many',\n",
              "  'want',\n",
              "  'say',\n",
              "  'information',\n",
              "  'us',\n",
              "  'go',\n",
              "  'must',\n",
              "  'going'],\n",
              " ['msg',\n",
              "  'gordon_banks',\n",
              "  'skepticism_chastity',\n",
              "  'surrender_soon',\n",
              "  'intellect_shameful',\n",
              "  'clinton',\n",
              "  'batf',\n",
              "  'sexual_orientation',\n",
              "  'evidence',\n",
              "  'cops'],\n",
              " ['sharks',\n",
              "  'bike',\n",
              "  'team',\n",
              "  'teams',\n",
              "  'season',\n",
              "  'hockey',\n",
              "  'nhl',\n",
              "  'inning',\n",
              "  'braves',\n",
              "  'lopez'],\n",
              " ['card',\n",
              "  'drive',\n",
              "  'motherboard',\n",
              "  'controller',\n",
              "  'scsi',\n",
              "  'best_offer',\n",
              "  'apple',\n",
              "  'windows',\n",
              "  'sale',\n",
              "  'monitor'],\n",
              " ['god',\n",
              "  'infallible',\n",
              "  'christians',\n",
              "  'christianity',\n",
              "  'catholic',\n",
              "  'church',\n",
              "  'iran',\n",
              "  'islam',\n",
              "  'resurrection',\n",
              "  'pope'],\n",
              " ['x',\n",
              "  'x_x',\n",
              "  'widget',\n",
              "  'program',\n",
              "  'xt',\n",
              "  'char',\n",
              "  'imake',\n",
              "  'string',\n",
              "  'exit',\n",
              "  'null'],\n",
              " ['card',\n",
              "  'monitor',\n",
              "  'best_offer',\n",
              "  'motherboard',\n",
              "  'adapter',\n",
              "  'dos',\n",
              "  'dram',\n",
              "  'duo',\n",
              "  'video',\n",
              "  'drive'],\n",
              " ['israel',\n",
              "  'israeli',\n",
              "  'god',\n",
              "  'islam',\n",
              "  'arabs',\n",
              "  'church',\n",
              "  'arab',\n",
              "  'lebanese',\n",
              "  'resurrection',\n",
              "  'scripture'],\n",
              " ['jesus',\n",
              "  'matthew',\n",
              "  'prophecy',\n",
              "  'god',\n",
              "  'holy_spirit',\n",
              "  'isaiah',\n",
              "  'db',\n",
              "  'hanging',\n",
              "  'israel',\n",
              "  'messiah'],\n",
              " ['us',\n",
              "  'unk',\n",
              "  'said',\n",
              "  'apartment',\n",
              "  'went',\n",
              "  'henrik',\n",
              "  'bullets',\n",
              "  'azerbaijan',\n",
              "  'woman',\n",
              "  'say']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZBTF_pXT_YB",
        "colab_type": "code",
        "outputId": "67d30772-68c3-45a0-ae69-322d07563c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "cm = CoherenceModel(topics=topic_words, texts = data_test,dictionary=dictionary, coherence='c_v')\n",
        "coherence = cm.get_coherence()\n",
        "print(coherence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHHqhFjmyyHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}