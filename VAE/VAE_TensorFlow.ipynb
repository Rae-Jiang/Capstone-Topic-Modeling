{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VAE_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rae-Jiang/Capstone-Topic-Modeling/blob/master/VAE/VAE_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED6QCEVU6INK",
        "colab_type": "text"
      },
      "source": [
        "# Variational Inference for Topic Models \n",
        "- Tensorflow version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIWmLOUd6PO5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5334056a-5f58-46e3-dac5-0fa4c34d777a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-YjLkRP2N4KY",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import itertools,time\n",
        "import sys, os\n",
        "from collections import OrderedDict,Counter\n",
        "from copy import deepcopy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "import sys, getopt\n",
        "import re\n",
        "import gensim\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "words = set(nltk.corpus.words.words())\n",
        "stops = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Agdk8TUrN4Kg",
        "colab": {}
      },
      "source": [
        "## data loading and preparation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pJh7kFvmN4Kl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6ca9f3d8-bef9-4364-edaa-e1cc61f5c96d"
      },
      "source": [
        "#load data\n",
        "data = pkl.load(open(\"data/sent_tokens.p\", \"rb\")) #list of tokenized words in each document\n",
        "bigram = gensim.models.Phrases(data)\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CENN5uVDGY3Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "40a42d90-4cf7-4885-a009-765e609e399b"
      },
      "source": [
        "def process_text(texts, with_bigram=True,filter_stopword_spacy = False, remove_non_eng = False,with_lemmatize = False, with_stem = False):\n",
        "    \"\"\"\n",
        "    Function to process texts. Following are the steps we take:\n",
        "    \n",
        "    1. Stopword Removal(nltk & spacy)\n",
        "    2. Collocation detection.\n",
        "    3. Lemmatization (not stem since stemming can reduce the interpretability).\n",
        "    4. Stem\n",
        "    \n",
        "    Parameters:\n",
        "    ----------\n",
        "    texts: Tokenized texts.\n",
        "    \n",
        "    Returns:\n",
        "    -------\n",
        "    texts: Pre-processed tokenized texts.\n",
        "    \"\"\"\n",
        "    # stopword removal using NLTK's english stopwords dataset remove non-english words.\n",
        "    texts = [[word for word in line if word not in stops and word.isalpha() ] for line in texts] \n",
        "    if remove_non_eng:\n",
        "        texts = [[word for word in line if word.lower() in words] for line in texts]\n",
        "    # Bigram collocation detection (frequently co-occuring tokens) using gensim's Phrases. can even try trigram collocation detection.\n",
        "    if with_bigram:\n",
        "        texts = [bigram[line] for line in texts]\n",
        "    #lemmatization (using gensim's lemmatize) to only keep the nouns. Lemmatization is generally better than stemming in the case of topic modeling since the words after lemmatization still remain understable. However, generally stemming might be preferred if the data is being fed into a vectorizer and isn't intended to be viewed. \n",
        "    if with_lemmatize:\n",
        "        texts = [[lemmatizer.lemmatize(word) for word in line] for line in texts] \n",
        "    if with_stem:\n",
        "        texts = [[porter.stem(word) for word in line] for line in texts] \n",
        "    if filter_stopword_spacy:\n",
        "        texts = [[word for word in line if not nlp.vocab[word].is_stop] for line in texts] \n",
        "    return texts\n",
        "\n",
        "data0 = process_text(data, with_bigram=True)\n",
        "data1 = process_text(data, with_bigram = True, filter_stopword_spacy = True)\n",
        "data2 = process_text(data, with_bigram = True, filter_stopword_spacy = True, remove_non_eng = True)\n",
        "data3 = process_text(data, with_bigram = True, filter_stopword_spacy = True, remove_non_eng = False, with_lemmatize = True)\n",
        "data4 = process_text(data, with_bigram = True, filter_stopword_spacy = True, remove_non_eng = True, with_lemmatize = True)\n",
        "data5 = process_text(data, with_bigram = True, filter_stopword_spacy = True, remove_non_eng = False,with_lemmatize = False, with_stem = True)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-9cd251064529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mdata0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_bigram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_stopword_spacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_bigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_stopword_spacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_non_eng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-127-9cd251064529>\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(texts, with_bigram, filter_stopword_spacy, remove_non_eng, with_lemmatize, with_stem)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Bigram collocation detection (frequently co-occuring tokens) using gensim's Phrases. can even try trigram collocation detection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwith_bigram\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#lemmatization (using gensim's lemmatize) to only keep the nouns. Lemmatization is generally better than stemming in the case of topic modeling since the words after lemmatization still remain understable. However, generally stemming might be preferred if the data is being fed into a vectorizer and isn't intended to be viewed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwith_lemmatize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-127-9cd251064529>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Bigram collocation detection (frequently co-occuring tokens) using gensim's Phrases. can even try trigram collocation detection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwith_bigram\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#lemmatization (using gensim's lemmatize) to only keep the nouns. Lemmatization is generally better than stemming in the case of topic modeling since the words after lemmatization still remain understable. However, generally stemming might be preferred if the data is being fed into a vectorizer and isn't intended to be viewed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwith_lemmatize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[0mnew_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py\u001b[0m in \u001b[0;36manalyze_sentence\u001b[0;34m(self, sentence, threshold, common_terms, scorer)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mis_common\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_common\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlast_uncommon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlast_uncommon\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0min_between\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;31m# test between last_uncommon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 score = self.score_item(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0QB0Gv-6INc",
        "colab_type": "code",
        "colab": {},
        "outputId": "72d29725-34fa-4a0b-9a9a-79c6b91fec61"
      },
      "source": [
        "#compare\n",
        "pd.DataFrame(list(zip(data[0][:20],data0[0][:20],data1[0][:20],data2[0][:20],data3[0][:20],data4[0][:20],data5[0][:20])), \n",
        "               columns =['original', 'with_bigram','filter_stopword_spacy','remove_non_eng','with_lemmatize','remove_non_eng&with_lemmatize','with_stem'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>with_bigram</th>\n",
              "      <th>filter_stopword_spacy</th>\n",
              "      <th>remove_non_eng</th>\n",
              "      <th>with_lemmatize</th>\n",
              "      <th>remove_non_eng&amp;with_lemmatize</th>\n",
              "      <th>with_stem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>charlotte</td>\n",
              "      <td>charlotte</td>\n",
              "      <td>charlotte</td>\n",
              "      <td>reaching</td>\n",
              "      <td>charlotte</td>\n",
              "      <td>reaching</td>\n",
              "      <td>charlott</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>hilton</td>\n",
              "      <td>hilton</td>\n",
              "      <td>hilton</td>\n",
              "      <td>goal</td>\n",
              "      <td>hilton</td>\n",
              "      <td>goal</td>\n",
              "      <td>hilton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>andersen</td>\n",
              "      <td>andersen</td>\n",
              "      <td>andersen</td>\n",
              "      <td>actually</td>\n",
              "      <td>andersen</td>\n",
              "      <td>actually</td>\n",
              "      <td>andersen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>dec</td>\n",
              "      <td>dec</td>\n",
              "      <td>dec</td>\n",
              "      <td>switch</td>\n",
              "      <td>dec</td>\n",
              "      <td>switch</td>\n",
              "      <td>dec</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>reaching</td>\n",
              "      <td>reaching</td>\n",
              "      <td>default</td>\n",
              "      <td>reaching</td>\n",
              "      <td>default</td>\n",
              "      <td>reach</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>you</td>\n",
              "      <td>goal</td>\n",
              "      <td>goal</td>\n",
              "      <td>setting</td>\n",
              "      <td>goal</td>\n",
              "      <td>setting</td>\n",
              "      <td>goal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>’re</td>\n",
              "      <td>actually</td>\n",
              "      <td>actually</td>\n",
              "      <td>critical</td>\n",
              "      <td>actually</td>\n",
              "      <td>critical</td>\n",
              "      <td>actual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>not</td>\n",
              "      <td>make</td>\n",
              "      <td>switch</td>\n",
              "      <td>curious</td>\n",
              "      <td>switch</td>\n",
              "      <td>curious</td>\n",
              "      <td>switch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>reaching</td>\n",
              "      <td>one</td>\n",
              "      <td>default</td>\n",
              "      <td>stop</td>\n",
              "      <td>default</td>\n",
              "      <td>stop</td>\n",
              "      <td>default</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>your</td>\n",
              "      <td>switch</td>\n",
              "      <td>setting</td>\n",
              "      <td>trying</td>\n",
              "      <td>setting</td>\n",
              "      <td>trying</td>\n",
              "      <td>set</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>goal</td>\n",
              "      <td>default</td>\n",
              "      <td>critical</td>\n",
              "      <td>good</td>\n",
              "      <td>critical</td>\n",
              "      <td>good</td>\n",
              "      <td>critic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>because</td>\n",
              "      <td>setting</td>\n",
              "      <td>curious</td>\n",
              "      <td>people</td>\n",
              "      <td>curious</td>\n",
              "      <td>people</td>\n",
              "      <td>curiou</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>you</td>\n",
              "      <td>critical</td>\n",
              "      <td>stop</td>\n",
              "      <td>focus</td>\n",
              "      <td>stop</td>\n",
              "      <td>focus</td>\n",
              "      <td>stop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>did</td>\n",
              "      <td>curious</td>\n",
              "      <td>trying</td>\n",
              "      <td>energy</td>\n",
              "      <td>trying</td>\n",
              "      <td>energy</td>\n",
              "      <td>tri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>n’t</td>\n",
              "      <td>stop</td>\n",
              "      <td>good</td>\n",
              "      <td>ladder</td>\n",
              "      <td>good</td>\n",
              "      <td>ladder</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>actually</td>\n",
              "      <td>trying</td>\n",
              "      <td>istock</td>\n",
              "      <td>work</td>\n",
              "      <td>istock</td>\n",
              "      <td>work</td>\n",
              "      <td>everyth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>make</td>\n",
              "      <td>good</td>\n",
              "      <td>people</td>\n",
              "      <td>false</td>\n",
              "      <td>people</td>\n",
              "      <td>false</td>\n",
              "      <td>istock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>one</td>\n",
              "      <td>everything</td>\n",
              "      <td>strengths</td>\n",
              "      <td>belief</td>\n",
              "      <td>strength</td>\n",
              "      <td>belief</td>\n",
              "      <td>peopl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>switch</td>\n",
              "      <td>istock</td>\n",
              "      <td>focus</td>\n",
              "      <td>good</td>\n",
              "      <td>focus</td>\n",
              "      <td>good</td>\n",
              "      <td>strength</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>your</td>\n",
              "      <td>people</td>\n",
              "      <td>energy</td>\n",
              "      <td>spending</td>\n",
              "      <td>energy</td>\n",
              "      <td>spending</td>\n",
              "      <td>focu</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     original with_bigram filter_stopword_spacy remove_non_eng with_lemmatize  \\\n",
              "0   charlotte   charlotte             charlotte       reaching      charlotte   \n",
              "1      hilton      hilton                hilton           goal         hilton   \n",
              "2    andersen    andersen              andersen       actually       andersen   \n",
              "3         dec         dec                   dec         switch            dec   \n",
              "4          19    reaching              reaching        default       reaching   \n",
              "5         you        goal                  goal        setting           goal   \n",
              "6         ’re    actually              actually       critical       actually   \n",
              "7         not        make                switch        curious         switch   \n",
              "8    reaching         one               default           stop        default   \n",
              "9        your      switch               setting         trying        setting   \n",
              "10       goal     default              critical           good       critical   \n",
              "11    because     setting               curious         people        curious   \n",
              "12        you    critical                  stop          focus           stop   \n",
              "13        did     curious                trying         energy         trying   \n",
              "14        n’t        stop                  good         ladder           good   \n",
              "15   actually      trying                istock           work         istock   \n",
              "16       make        good                people          false         people   \n",
              "17        one  everything             strengths         belief       strength   \n",
              "18     switch      istock                 focus           good          focus   \n",
              "19       your      people                energy       spending         energy   \n",
              "\n",
              "   remove_non_eng&with_lemmatize with_stem  \n",
              "0                       reaching  charlott  \n",
              "1                           goal    hilton  \n",
              "2                       actually  andersen  \n",
              "3                         switch       dec  \n",
              "4                        default     reach  \n",
              "5                        setting      goal  \n",
              "6                       critical    actual  \n",
              "7                        curious    switch  \n",
              "8                           stop   default  \n",
              "9                         trying       set  \n",
              "10                          good    critic  \n",
              "11                        people    curiou  \n",
              "12                         focus      stop  \n",
              "13                        energy       tri  \n",
              "14                        ladder      good  \n",
              "15                          work   everyth  \n",
              "16                         false    istock  \n",
              "17                        belief     peopl  \n",
              "18                          good  strength  \n",
              "19                      spending      focu  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-V9o3qYgFt8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "efbfdcee-0c43-4a28-8e3e-0df858049997"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F2p3btcgi_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "basedir = '/content/drive/My Drive/1006 Capstone/Rui'\n",
        "data_folder_path = os.path.join(basedir, 'data')\n",
        "res_folder_path = os.path.join(basedir, 'result')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGBIAWHa6INg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pkl.dump(data0, open(os.path.join(data_folder_path,'sent_tokens_bi.p'),'wb'))\n",
        "pkl.dump(data1, open(os.path.join(data_folder_path,'sent_tokens_bi_spacy.p'),'wb'))\n",
        "pkl.dump(data2, open(os.path.join(data_folder_path,'sent_tokens_bi_spacy_eng.p'),'wb'))\n",
        "pkl.dump(data3, open(os.path.join(data_folder_path,'sent_tokens_bi_spacy_lem.p'),'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhonlX-d6tCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pkl.load(open('data/sent_tokens.p','rb'))\n",
        "data0= pkl.load(open('data/sent_tokens_bi.p','rb'))\n",
        "data1= pkl.load(open('data/sent_tokens_bi_spacy.p','rb'))\n",
        "data2= pkl.load(open('data/sent_tokens_bi_spacy_eng.p','rb'))\n",
        "data3= pkl.load(open('data/sent_tokens_bi_spacy_lem.p','rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b64LuVSI6INl",
        "colab_type": "text"
      },
      "source": [
        "### train test split & vocabulary prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "afiZWavtEwt5",
        "colab": {}
      },
      "source": [
        "data = data0 #basic preprocessed\n",
        "# split dataset into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "\n",
        "all_tokens = []\n",
        "for i in train:\n",
        "    all_tokens += i\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qigSEqxGN4Kr",
        "colab": {}
      },
      "source": [
        "def build_vocab(all_tokens, max_vocab_size):\n",
        "    '''\n",
        "    Returns:\n",
        "     id2token, token2id\n",
        "    ''' \n",
        "    token_counter = Counter(all_tokens)\n",
        "    # unzip the vocab and its corresponding count\n",
        "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
        "    id2token = list(vocab)\n",
        "    # Give indices from 2 to the vocab\n",
        "    token2id = dict(zip(vocab, range(2, 2+len(vocab))))\n",
        "    # Add pad and unk to vocab\n",
        "    id2token = ['<pad>', '<unk>'] + id2token\n",
        "    token2id['<pad>'] = PAD_IDX\n",
        "    token2id['<unk>'] = UNK_IDX\n",
        "    return token2id, id2token\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqiB_wPr6INr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "fc939d3d-4163-40f4-fe32-de5cd4e616cc"
      },
      "source": [
        "max_vocab_size = 15000\n",
        "# save index 0 for unk and 1 for pad\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "\n",
        "token2id, id2token = build_vocab(all_tokens,max_vocab_size) \n",
        "id2token[:20]"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " '<unk>',\n",
              " 'one',\n",
              " 'said',\n",
              " 'would',\n",
              " 'also',\n",
              " 'time',\n",
              " 'people',\n",
              " 'new',\n",
              " 'like',\n",
              " 'may',\n",
              " 'get',\n",
              " 'use',\n",
              " 'could',\n",
              " 'first',\n",
              " 'make',\n",
              " 'information',\n",
              " 'us',\n",
              " 'even',\n",
              " 'two']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "52FDOTl0N4Kt",
        "colab": {}
      },
      "source": [
        "def token2index_dataset(tokens_data, token2id, id2token):\n",
        "    indices_data = []\n",
        "    for tokens in tokens_data:\n",
        "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
        "        indices_data.append(index_list)\n",
        "    return indices_data\n",
        "\n",
        "x_train = token2index_dataset(train, token2id, id2token)\n",
        "x_train = np.array([np.array(document) for document in x_train])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-3YdwMuJN4Ky",
        "colab": {}
      },
      "source": [
        "def onehot(data, min_length):\n",
        "    return np.bincount(data, minlength=min_length)\n",
        "\n",
        "x_train = np.array([onehot(doc.astype('int'),max_vocab_size+2) for doc in x_train if np.sum(doc)!=0])\n",
        "n_samples_tr = x_train.shape[0]\n",
        "docs_tr = x_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q9ioaUVYN4K0",
        "colab": {}
      },
      "source": [
        "x_val = token2index_dataset(test, token2id, id2token)\n",
        "x_val = np.array([np.array(document) for document in x_val])\n",
        "x_val = np.array([onehot(doc.astype('int'),max_vocab_size+2) for doc in x_val if np.sum(doc)!=0])\n",
        "n_samples_te = x_val.shape[0]\n",
        "docs_te = x_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hcBjuFINN4K_",
        "colab": {}
      },
      "source": [
        "## Model and Train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hPz93SCeN4Ke",
        "colab": {}
      },
      "source": [
        "slim = tf.contrib.slim\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class VAE(object):\n",
        "    \"\"\"\n",
        "    See \"Auto-Encoding Variational Bayes\" by Kingma and Welling for more details.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, network_architecture, transfer_fct=tf.nn.softplus,\n",
        "                 learning_rate=0.01, batch_size=100):\n",
        "        self.network_architecture = network_architecture\n",
        "        self.transfer_fct = transfer_fct\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        print('Initial Learning Rate:', self.learning_rate)\n",
        "\n",
        "        # tf Graph input\n",
        "        self.x = tf.placeholder(tf.float32, [None, network_architecture[\"n_input\"]], name='input')\n",
        "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "        self.h_dim = (network_architecture[\"n_z\"]) # had a float before\n",
        "        self.a = 1*np.ones((1 , self.h_dim)).astype(np.float32)                         # a    = 1\n",
        "        self.prior_mean = tf.constant((np.log(self.a).T-np.mean(np.log(self.a),1)).T)          # prior_mean  = 0\n",
        "        self.prior_var = tf.constant(  ( ( (1.0/self.a)*( 1 - (2.0/self.h_dim) ) ).T +       # prior_var = 0.99 + 0.005 = 0.995\n",
        "                                ( 1.0/(self.h_dim*self.h_dim) )*np.sum(1.0/self.a,1) ).T  )\n",
        "        self.prior_logvar = tf.log(self.prior_var)\n",
        "        self.means = []\n",
        "\n",
        "        self._create_network()\n",
        "        with tf.name_scope('cost'):\n",
        "            self._create_loss_optimizer()\n",
        "\n",
        "        init = tf.initialize_all_variables()\n",
        "\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(init)\n",
        "\n",
        "    def _create_network(self):\n",
        "        \"\"\"\n",
        "        steps:\n",
        "        1. initialize weights\n",
        "        2. build recognition network\n",
        "        3. build reconstruction network\n",
        "        \"\"\"\n",
        "        n_z = self.network_architecture['n_z']\n",
        "        n_hidden_gener_1 = self.network_architecture['n_hidden_gener_1']\n",
        "        en1 = slim.layers.linear(self.x, self.network_architecture['n_hidden_recog_1'], scope='FC_en1')\n",
        "        en1 = tf.nn.softplus(en1, name='softplus1')\n",
        "        en2 = slim.layers.linear(en1,    self.network_architecture['n_hidden_recog_2'], scope='FC_en2')\n",
        "        en2 = tf.nn.softplus(en2, name='softplus2')\n",
        "        en2_do = slim.layers.dropout(en2, self.keep_prob, scope='en2_dropped')\n",
        "        self.posterior_mean   = slim.layers.linear(en2_do, self.network_architecture['n_z'], scope='FC_mean')\n",
        "        self.posterior_logvar = slim.layers.linear(en2_do, self.network_architecture['n_z'], scope='FC_logvar')\n",
        "        self.posterior_mean   = slim.layers.batch_norm(self.posterior_mean, scope='BN_mean')\n",
        "        self.posterior_logvar = slim.layers.batch_norm(self.posterior_logvar, scope='BN_logvar')\n",
        "        \n",
        "        with tf.name_scope('z_scope'):\n",
        "            eps = tf.random_normal((self.batch_size, n_z), 0, 1,                            # take noise\n",
        "                                   dtype=tf.float32)\n",
        "            self.z = tf.add(self.posterior_mean,\n",
        "                            tf.multiply(tf.sqrt(tf.exp(self.posterior_logvar)), eps))         # reparameterization z\n",
        "            self.posterior_var = tf.exp(self.posterior_logvar) \n",
        "\n",
        "        self.p = slim.layers.softmax(self.z)\n",
        "        p_do = slim.layers.dropout(self.p, self.keep_prob, scope='p_dropped')               # dropout(softmax(z))\n",
        "        decoded = slim.layers.linear(p_do, n_hidden_gener_1, scope='FC_decoder')\n",
        "\n",
        "        self.x_reconstr_mean = tf.nn.softmax(slim.layers.batch_norm(decoded, scope='BN_decoder'))                    # softmax(bn(50->1995))\n",
        "\n",
        "        print(self.x_reconstr_mean)\n",
        "\n",
        "    def _create_loss_optimizer(self):\n",
        "\n",
        "        tensor = self.x * tf.log(self.x_reconstr_mean+1e-10)                                                   # prevent log(0)\n",
        "        indices = [i for i in range(1,tensor.shape[1])] # exclude 'pad', include 'unk'\n",
        "        result = tf.gather(tensor, indices, axis=1)\n",
        "        NL = -tf.reduce_sum(result, 1)\n",
        "        # NL = -tf.reduce_sum(self.x * tf.log(self.x_reconstr_mean+1e-10), 1)     # cross entropy on categorical:- sum(ylog(p))\n",
        "\n",
        "        var_division    = self.posterior_var  / self.prior_var\n",
        "        diff            = self.posterior_mean - self.prior_mean\n",
        "        diff_term       = diff * diff / self.prior_var\n",
        "        logvar_division = self.prior_logvar - self.posterior_logvar\n",
        "        KLD = 0.5 * (tf.reduce_sum(var_division + diff_term + logvar_division, 1) - self.h_dim )\n",
        "\n",
        "        self.cost = tf.reduce_mean(NL + KLD)\n",
        " \n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.99,epsilon=0.01).minimize(self.cost)\n",
        "\n",
        "    def partial_fit(self, X):\n",
        "\n",
        "        #if hasattr(self, 'decoder_weight'):\n",
        "            #decoder_weight = self.decoder_weight\n",
        "        #else:\n",
        "        decoder_weight = [v for v in tf.global_variables() if v.name=='FC_decoder/weights:0'][0]\n",
        "        opt, cost,emb,p = self.sess.run((self.optimizer, self.cost, decoder_weight, self.p),feed_dict={self.x: X,self.keep_prob: .8})\n",
        "        # print(self.sess.run((self.p),feed_dict={self.x: X,self.keep_prob: .8}))\n",
        "        return cost,emb,p\n",
        "\n",
        "    def test(self, X):\n",
        "        \"\"\"Test the model and return the lowerbound on the log-likelihood.\n",
        "        \"\"\"\n",
        "        cost = self.sess.run((self.cost),feed_dict={self.x: np.expand_dims(X, axis=0),self.keep_prob: 1.0})\n",
        "        return cost\n",
        "    def topic_prop(self, X):\n",
        "        \"\"\"heta_ is the topic proportion vector. Apply softmax transformation to it before use.\n",
        "        \"\"\"\n",
        "        theta_ = self.sess.run((self.z),feed_dict={self.x: np.expand_dims(X, axis=0),self.keep_prob: 1.0})\n",
        "        return theta_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YaUALWnBN4K5",
        "colab": {}
      },
      "source": [
        "def create_minibatch(data,batch_size):\n",
        "    rng = np.random.RandomState(10)\n",
        "\n",
        "    while True:\n",
        "        # Return random data samples of a size 'minibatch_size' at each iteration\n",
        "        ixs = rng.randint(data.shape[0], size=batch_size)\n",
        "        yield data[ixs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsxKHfmyN4K9",
        "colab": {}
      },
      "source": [
        "def print_top_words(beta, feature_names, n_top_words=10):\n",
        "    print('---------------Printing the Topics------------------')\n",
        "    for i in range(len(beta)):\n",
        "        print(\" \".join([feature_names[j]\n",
        "            for j in beta[i].argsort()[:-n_top_words - 1:-1]]))\n",
        "    print('---------------End of Topics------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E_jExRbwN4LK",
        "colab": {}
      },
      "source": [
        "def cal_val_ppl(model,val):\n",
        "    cost=[]\n",
        "    for doc in val:\n",
        "        doc = doc.astype('float32')\n",
        "        n_d = np.sum(doc[1:]) # count non-pad\n",
        "        c=model.test(doc)\n",
        "        if n_d==0:\n",
        "            continue\n",
        "        else:\n",
        "            cost.append(c/n_d)\n",
        "    # print('The approximated perplexity for test set is: ',np.exp(np.mean(np.array(cost))))\n",
        "    return np.exp(np.mean(np.array(cost)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RCA-d2_gN4K7",
        "colab": {}
      },
      "source": [
        "def model_train(network_architecture, minibatches, val, learning_rate=0.01,\n",
        "          batch_size=200, training_epochs=100, display_step=5):\n",
        "    tf.reset_default_graph()\n",
        "    vae = VAE(network_architecture,transfer_fct=tf.nn.softplus,\n",
        "                learning_rate=learning_rate, batch_size=batch_size)\n",
        "    writer = tf.summary.FileWriter('logs', tf.get_default_graph())\n",
        "    emb=0\n",
        "    # Training cycle\n",
        "    best_val_ppl = 10000\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(n_samples_tr / batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_xs = next(minibatches)\n",
        "            # Fit training using batch data\n",
        "            cost,emb,p = vae.partial_fit(batch_xs)\n",
        "            # Compute average loss\n",
        "            avg_cost += cost / n_samples_tr * batch_size\n",
        "\n",
        "            if np.isnan(avg_cost):\n",
        "                print(epoch,i,np.sum(batch_xs,1).astype(np.int),batch_xs.shape)\n",
        "                print('Encountered NaN, stopping training. Please check the learning_rate settings and the momentum.')\n",
        "                # return vae,emb\n",
        "                sys.exit()\n",
        "        # record best val ppl\n",
        "        val_ppl = cal_val_ppl(vae,val)\n",
        "        best_val_ppl = min(best_val_ppl,val_ppl)\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \\\n",
        "                  \"avg train cost=\", \"{:.9f} approximated test PPL is: {:.9f}\".format(avg_cost,val_ppl))\n",
        "            \n",
        "    return vae,emb,best_val_ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rVJtm0neN4LD",
        "colab": {}
      },
      "source": [
        "def search_best_params(data, max_vocab_size, learning_rate, batch_size, layer1, layer2, num_topics, epochs):\n",
        "    #train,test split \n",
        "    train, test = train_test_split(data, test_size=0.2)\n",
        "    #build vocab for train and index for train,test\n",
        "    all_tokens = []\n",
        "    for i in train:\n",
        "        all_tokens += i\n",
        "    token2id, id2token = build_vocab(all_tokens,max_vocab_size)\n",
        "\n",
        "    x_train = token2index_dataset(train, token2id, id2token)\n",
        "    x_train = np.array([np.array(document) for document in x_train]) \n",
        "    x_train = np.array([onehot(doc.astype('int'),max_vocab_size+2) for doc in x_train if np.sum(doc)!=0])\n",
        "    n_samples_tr = x_train.shape[0]\n",
        "    x_val = token2index_dataset(test, token2id, id2token)\n",
        "    x_val = np.array([np.array(document) for document in x_val])\n",
        "    x_val = np.array([onehot(doc.astype('int'),max_vocab_size+2) for doc in x_val if np.sum(doc)!=0])\n",
        "\n",
        "    #collate batches\n",
        "    tf.reset_default_graph()\n",
        "    network_architecture = \\\n",
        "        dict(n_hidden_recog_1=layer1, # 1st layer encoder neurons\n",
        "             n_hidden_recog_2=layer2, # 2nd layer encoder neurons\n",
        "             n_hidden_gener_1=x_train.shape[1], # 1st layer decoder neurons\n",
        "             n_input=x_train.shape[1], # MNIST data input (img shape: 28*28)\n",
        "             n_z=num_topics)  # dimensionality of latent space\n",
        "\n",
        "    minibatches = create_minibatch(x_train.astype('float32'),batch_size=batch_size)\n",
        "\n",
        "    return model_train(network_architecture, minibatches,x_val,learning_rate,batch_size, training_epochs=epochs, display_step=5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwMc-faLWQiv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "11831806-da39-419f-da51-4b933c618d61"
      },
      "source": [
        "#tune hyper-params and measure time efficiency\n",
        "import time\n",
        "res = []\n",
        "list_data = [data,data0,data1,data2,data3]\n",
        "for i in range(len(list_data)):\n",
        "    DATA = list_data[i]\n",
        "    for MAX_VOCAB in list(range(5000,17000,2000)):\n",
        "        for lr in [0.01,0.008,0.005,0.002]:\n",
        "            for BATCH_SIZE in [64,128,160,200]:\n",
        "                for LAYER1 in [60,100,200,400,]:\n",
        "                    if LAYER1 == 60:\n",
        "                        LAYER2 = int(LAYER1//1.5)\n",
        "                    else:\n",
        "                        LAYER2 = LAYER1//2\n",
        "                    time1 = time.time()\n",
        "                    vae, emb, best_val_ppl = search_best_params(DATA, MAX_VOCAB, learning_rate=lr,\n",
        "                                                    batch_size=BATCH_SIZE,layer1=LAYER1, layer2=LAYER2, num_topics=20, epochs=100)\n",
        "                    time_span = time.time() - time1\n",
        "                    print('best_val_ppl:',best_val_ppl)\n",
        "                    record = \\\n",
        "                        dict(dataset=i-1, \n",
        "                             max_vocab_size=MAX_VOCAB,\n",
        "                             learning_rate=lr, \n",
        "                             batch_size=BATCH_SIZE, \n",
        "                             layer1_size = LAYER1,\n",
        "                             layer2_size = LAYER2,\n",
        "                             vae = vae, emb = emb, \n",
        "                             best_val_ppl = best_val_ppl,\n",
        "                             time = time_span) \n",
        "                    res.append(record)\n",
        "\n",
        "pkl.dump(res, open(os.path.join(res_folder_path,'res_tune.p'),'wb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Learning Rate: 0.01\n",
            "Tensor(\"Softmax_1:0\", shape=(64, 5002), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 avg train cost= 2619.614476562 approximated test PPL is: 2710.492919922\n",
            "Epoch: 0006 avg train cost= 2055.112280273 approximated test PPL is: 560.703186035\n",
            "Epoch: 0011 avg train cost= 1991.450687500 approximated test PPL is: 538.786560059\n",
            "Epoch: 0016 avg train cost= 2010.081862305 approximated test PPL is: 530.249633789\n",
            "Epoch: 0021 avg train cost= 1975.214496094 approximated test PPL is: 530.424621582\n",
            "Epoch: 0026 avg train cost= 1942.368915039 approximated test PPL is: 523.732788086\n",
            "Epoch: 0031 avg train cost= 1993.663530273 approximated test PPL is: 529.871765137\n",
            "Epoch: 0036 avg train cost= 1955.361846680 approximated test PPL is: 534.825439453\n",
            "Epoch: 0041 avg train cost= 1931.640883789 approximated test PPL is: 533.190734863\n",
            "Epoch: 0046 avg train cost= 1930.169207031 approximated test PPL is: 533.718811035\n",
            "Epoch: 0051 avg train cost= 1937.376344727 approximated test PPL is: 537.507385254\n",
            "Epoch: 0056 avg train cost= 1943.940034180 approximated test PPL is: 540.214050293\n",
            "Epoch: 0061 avg train cost= 1934.005400391 approximated test PPL is: 540.062377930\n",
            "Epoch: 0066 avg train cost= 1990.648784180 approximated test PPL is: 533.573486328\n",
            "Epoch: 0071 avg train cost= 1960.107430664 approximated test PPL is: 535.142272949\n",
            "Epoch: 0076 avg train cost= 2005.030215820 approximated test PPL is: 526.266967773\n",
            "Epoch: 0081 avg train cost= 1944.131928711 approximated test PPL is: 526.769348145\n",
            "Epoch: 0086 avg train cost= 1967.071374023 approximated test PPL is: 531.021057129\n",
            "Epoch: 0091 avg train cost= 1916.944606445 approximated test PPL is: 522.949157715\n",
            "Epoch: 0096 avg train cost= 1939.637457031 approximated test PPL is: 536.700134277\n",
            "best_val_ppl: 521.7825\n",
            "Initial Learning Rate: 0.01\n",
            "Tensor(\"Softmax_1:0\", shape=(64, 5002), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 avg train cost= 2745.413977539 approximated test PPL is: 2710.861328125\n",
            "Epoch: 0006 avg train cost= 2089.366898437 approximated test PPL is: 634.089050293\n",
            "Epoch: 0011 avg train cost= 1992.422839844 approximated test PPL is: 502.629791260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-ba9Fb2Bcakj",
        "colab": {}
      },
      "source": [
        "def gen_minibatch_whole_train(data):\n",
        "    c = 0\n",
        "    while True:\n",
        "        # Return random data samples of a size 'minibatch_size' at each iteration\n",
        "        c += 1\n",
        "        yield data[200*(c-1):200*c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9bl9Cba7c-xr",
        "colab": {}
      },
      "source": [
        "batch_size=200\n",
        "minibatches = gen_minibatch_whole_train(docs_tr.astype('float32'))\n",
        "whole_train = int(n_samples_tr / batch_size)\n",
        "x_var_mean = []\n",
        "for i in range(whole_train):\n",
        "    batch_xs = next(minibatches)\n",
        "    # Fit training using batch data\n",
        "    cost,emb,p = vae.partial_fit(batch_xs)\n",
        "    x_var_mean += p.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CyyZMynPdYji",
        "colab": {}
      },
      "source": [
        "x_var_mean = np.array(x_var_mean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lcOz--9dfYTm",
        "colab": {}
      },
      "source": [
        "def top_words(beta, feature_names, n_top_words=10):\n",
        "    res = []\n",
        "    for i in range(len(beta)):\n",
        "        res.append([feature_names[j] for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "acxSgrm7fm7R",
        "colab": {}
      },
      "source": [
        "topic_words = top_words(emb, list(zip(*sorted(token2id.items(), key=lambda x: x[1])))[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sSCAM0Z7N4LH",
        "colab": {}
      },
      "source": [
        "print_top_words(emb, list(zip(*sorted(token2id.items(), key=lambda x: x[1])))[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bw5ZEwrv_CBH",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import scale\n",
        "from sklearn.linear_model import Lasso\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "\n",
        "def build_lasso_graph(x, l1_lambda, topic_words,both=True,):\n",
        "    \"\"\"\n",
        "    A adaption from CTM's lasso-graph.r script\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x: numpy.array\n",
        "        N x K data matrix -- e.g., the variational means (\"final-lambda.dat\")\n",
        "    l1_lambda: float\n",
        "        relative bound on the l1-norm of the parameters, in [0,1]\n",
        "    and: bool\n",
        "        if and=T/F then the graph is computed by taking the intersction/union of the nbhds\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    ihat: numpy.array \n",
        "        K x K adjacency matrix of the topic graph\n",
        "    \"\"\"\n",
        "    x = scale(x)\n",
        "    topic_count = Counter(x.argmax(axis=1))\n",
        "    N, K = x.shape\n",
        "    Shat = np.zeros((K,K), dtype=bool)\n",
        "    print('Parameters:')\n",
        "    print(f'N={N}, K={K}, lambda={l1_lambda}')\n",
        "    print()\n",
        "    print('Fitting...')\n",
        "    for j in range(K):\n",
        "        column_mask = np.ones(K, dtype=bool)\n",
        "        column_mask[j] = False\n",
        "        # The response is the j-th column\n",
        "        y = x[:,j]\n",
        "        X = x[:,column_mask]\n",
        "        # Do the l1-regularized regression\n",
        "        # Note: the bound in l1ce code is the upper bound on the l1\n",
        "        # norm.  So, a larger bound is a weaker constraint on the model\n",
        "        lasso_model = Lasso(\n",
        "            normalize=False,\n",
        "            alpha=l1_lambda,\n",
        "            tol=1e-6,\n",
        "            #positive=True,\n",
        "            max_iter=10000,\n",
        "        )\n",
        "        lasso_model.fit(X, y)\n",
        "        indices = np.array(range(K))[column_mask]\n",
        "        beta = lasso_model.coef_\n",
        "        nonzero = indices[beta>0]\n",
        "        Shat[j, nonzero] = True\n",
        "        Shat[j,j] = True\n",
        "    print('Fitting completed!')\n",
        "    # Include an edge if either (and=F) or both (and=T) endpoints are neighbors\n",
        "    Ihat = np.zeros((K,K), dtype=bool)\n",
        "    if both is True:\n",
        "        for i in range(K):\n",
        "            Ihat[:, i] = Shat[:,i]&Shat[i,:]\n",
        "    else:\n",
        "        for i in range(K):\n",
        "            Ihat[:, i] = Shat[:,i]|Shat[i,:]\n",
        "    # Visualize topic graph\n",
        "    # Construct Graph from adjacency matrix\n",
        "    G = nx.from_numpy_array(Ihat)\n",
        "    annotations = [f'({np.round(topic_count[i]*100/N, 2)}%)\\n'+\"\\n\".join(topic_word) for i, topic_word in enumerate(topic_words)]\n",
        "    node_label_dict = dict(zip(range(20), annotations))\n",
        "    pos = nx.spring_layout(G, k=0.3)\n",
        "    fig = plt.figure(figsize=(40,20))\n",
        "    plt.title(f'Number of topics: {K}, L1-Regularization Strength: {l1_lambda}')\n",
        "    nx.draw(G, pos, font_size=20, with_labels=True, labels=node_label_dict, node_size=5)\n",
        "    return Ihat, fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dBEkMVmRYlwr",
        "colab": {}
      },
      "source": [
        "topic_graph_adj_matrix, topic_graph_fig = build_lasso_graph(x_var_mean, 3e-1, topic_words, both=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ryWGK_DWAZFV",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}