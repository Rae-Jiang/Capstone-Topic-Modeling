{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vacab size 142900\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "tokens = pkl.load(open(\"data/capstone_data_sent_tokens.p\", \"rb\"))\n",
    "all_tokens = pkl.load(open(\"data/capstone_data_all_tokens.p\", \"rb\"))\n",
    "print(\"vacab size\", len(set(all_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MAX_VOCAB_SIZE = 100000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(MAX_VOCAB_SIZE))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#Global Parameters\n",
    "MAX_WORD_LENGTH = 800\n",
    "BATCH_SIZE = 64\n",
    "START_LETTER = '<s>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seqs, token2id):\n",
    "        \"\"\"\n",
    "        @param data_list: list of character\n",
    "        \"\"\"\n",
    "        self.data_list = seqs\n",
    "        self.token2id = token2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        token_idx = [self.token2id[c] if c in self.token2id.keys() else UNK_IDX  for c in self.data_list[key][:MAX_WORD_LENGTH]]\n",
    "        return [token_idx, len(token_idx)]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all\n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    length_list = []\n",
    "\n",
    "    for datum in batch:\n",
    "        length_list.append(datum[1])\n",
    "        \n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]),\n",
    "                                pad_width=((0,MAX_WORD_LENGTH-datum[1])),\n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    ind_dec_order = np.argsort(length_list)[::-1]  ##desc order for speed\n",
    "    data_list = np.array(data_list)[ind_dec_order]\n",
    "    length_list = np.array(length_list)[ind_dec_order]\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list)]\n",
    "\n",
    "train_dataset = VocabDataset(tokens, token2id)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Generator & Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Author       : William\n",
    "# @Project      : TextGAN-william\n",
    "# @URL          : https://github.com/williamSYSU/TextGAN-PyTorch\n",
    "# @Time         : Created at 2019-07-26\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# import config as cfg\n",
    "# from utils.helpers import truncated_normal_\n",
    "\n",
    "\n",
    "class LSTMGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, max_seq_len, padding_idx, gpu=False):\n",
    "        super(LSTMGenerator, self).__init__()\n",
    "        self.name = 'vanilla'\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.temperature = 1.0   \n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)  #\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, inp, hidden, need_hidden=False):\n",
    "        \"\"\"\n",
    "        Embeds input and applies LSTM\n",
    "        :param inp: batch_size * seq_len\n",
    "        :param hidden: (h, c)\n",
    "        :param need_hidden: if return hidden, use for sampling\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(inp)  # batch_size * len * embedding_dim\n",
    "        \n",
    "        if len(inp.size()) == 1:\n",
    "            emb = emb.unsqueeze(1)  # batch_size * 1 * embedding_dim\n",
    "\n",
    "        out, hidden = self.lstm(emb, hidden)  # out: batch_size * seq_len * hidden_dim\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)  # out: (batch_size * seq_len) * hidden_dim\n",
    "        out = self.lstm2out(out)  # (batch_size * seq_len) * vocab_size\n",
    "        out = self.temperature * out  # temperature\n",
    "        pred = self.softmax(out)\n",
    "\n",
    "        if need_hidden:\n",
    "            return pred, hidden\n",
    "        else:\n",
    "            return pred\n",
    "\n",
    "    def sample(self, num_samples, batch_size, start_letter=START_LETTER):\n",
    "        \"\"\"\n",
    "        Samples the network and returns num_samples samples of length max_seq_len.\n",
    "        :return samples: num_samples * max_seq_length (a sampled sequence in each row)\n",
    "        \"\"\"\n",
    "        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n",
    "        samples = torch.zeros(num_batch * batch_size, self.max_seq_len).long()\n",
    "\n",
    "        # Generate sentences with multinomial sampling strategy\n",
    "        for b in range(num_batch):\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "            inp = torch.LongTensor([start_letter] * batch_size)\n",
    "            if self.gpu:\n",
    "                inp = inp.cuda()\n",
    "\n",
    "            for i in range(self.max_seq_len):\n",
    "                out, hidden = self.forward(inp, hidden, need_hidden=True)  # out: num_samples * vocab_size\n",
    "                next_token = torch.multinomial(torch.exp(out), 1)  # num_samples * 1 (sampling from each row)\n",
    "                samples[b * batch_size:(b + 1) * batch_size, i] = next_token.view(-1)\n",
    "                inp = next_token.view(-1)\n",
    "        samples = samples[:num_samples]\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                torch.nn.init.normal_(param, std=stddev)\n",
    "\n",
    "    def init_hidden(self, batch_size=cfg.batch_size):\n",
    "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
    "\n",
    "        if self.gpu:\n",
    "            return h.cuda(), c.cuda()\n",
    "        else:\n",
    "            return h, c\n",
    "        \n",
    "class CNNDiscriminator(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, filter_sizes, num_filters, padding_idx, gpu=False,\n",
    "                 dropout=0.2):\n",
    "        super(CNNDiscriminator, self).__init__()\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.feature_dim = sum(num_filters)\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, n, (f, embed_dim)) for (n, f) in zip(num_filters, filter_sizes)\n",
    "        ])\n",
    "        #TODO batch normalization\n",
    "        self.highway = nn.Linear(self.feature_dim, self.feature_dim)\n",
    "        self.feature2out = nn.Linear(self.feature_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"\n",
    "        Get final predictions of discriminator\n",
    "        :param inp: batch_size * seq_len\n",
    "        :return: pred: batch_size * 2\n",
    "        \"\"\"\n",
    "        feature = self.get_feature(inp)\n",
    "        pred = self.feature2out(self.dropout(feature))\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def get_feature(self, inp):\n",
    "        \"\"\"\n",
    "        Get feature vector of given sentences\n",
    "        :param inp: batch_size * max_seq_len\n",
    "        :return: batch_size * feature_dim\n",
    "        \"\"\"\n",
    "        emb = self.embeddings(inp).unsqueeze(1)  # batch_size * 1 * max_seq_len * embed_dim\n",
    "        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs]  # [batch_size * num_filter * length]\n",
    "        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs]  # [batch_size * num_filter]\n",
    "        pred = torch.cat(pools, 1)  # tensor: batch_size * feature_dim\n",
    "        highway = self.highway(pred)\n",
    "        pred = torch.sigmoid(highway) * F.relu(highway) + (1. - torch.sigmoid(highway)) * pred  # highway\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                torch.nn.init.normal_(param, std=stddev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Model Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(256, 128, 4, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv_disc = nn.Conv2d(128, 100, 1)\n",
    "\n",
    "        self.conv_mu = nn.Conv2d(128, 1, 1)\n",
    "        self.conv_var = nn.Conv2d(128, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.1, inplace=True)\n",
    "\n",
    "        disc_logits = self.conv_disc(x).squeeze()\n",
    "\n",
    "        mu = self.conv_mu(x).squeeze()\n",
    "        var = torch.exp(self.conv_var(x).squeeze())\n",
    "\n",
    "        return disc_logits, mu, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\min_{G,Q} \\max_{D} E_{x \\sim P_{data}}[\\log(D(x))]+E_{z \\sim P_z, c \\sim P_c}[\\log(1-D(G(z,c)))-\\lambda Q(c | G(z,c))]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
